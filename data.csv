most applications of deep learning use deep convolutional neural networks (cnn). such architectures are among the most promising and have brought breakthroughs in processing images, video, speech and audio [94] and today, it is widely adopted by the computer vision community [95,96]. additional architectures include recurrent neural networks, which are particularly useful for processing sequential data such as text and speech. one important further development is to augment these networks with an explicit memory, e.g., by means of the long short-term memory (lstm) [97] that uses special hidden units, the natural behaviour of which is to remember inputs for a long time. in addition to neural networks, deep learning can also be realized within other frameworks. for instance, deep forests with a decision tree ensemble is proposed in zhou et al. (2017) [98], which shows performance highly competitive to deep neural networks in a broad range of tasks. several deep learning libraries are out there, supporting to build deep learning algorithms of considerable complexity. particularly popular are, for instance, tensorflow, keras, caffe, and torch, mostly using python [99]. many practical recommendations are given in [100] for successfully and efficiently training and debugging large-scale and deep neural networks. despite the existence of these deep learning libraries, there is still a lack of comprehensible and easy-to-use high-level tools for the design, training, and testing of deep neural networks. a step towards such tools is the recently proposed system barista [101], an open-source graphical high-level interface for the caffe deep learning library.traditionally, hand-crafted features are used for classification, which requires considerable domain expertise and careful engineering. the recent development of deep learning [91,92,93] is currently very promising on automatic feature learning. in fact, a hierarchy of features can be learned to build representations of patterns with multiple levels of abstraction. for understanding images, for instance, the first layer typically learns low-level features like edges in the image. the second layer detects middle-level motifs by spotting  particular arrangements of edges. then, the next layer may assemble motifs into larger combinations that correspond to object parts, and subsequent layers would detect objects as combinations of these parts. importantly, these layers of features are not designed by engineers, but learned instead from data using a learning procedure. in essence, deep learning methods are representation-learning methods with multiple levels of representation. nowadays, smart healthcare has appeared to be an interdisciplinary subject by integrating mixed computing techniques into the health administration [1, 2]. the primary purpose of smart healthcare is to offer pervasive and personalized medical services and health protection to people. computer-aided diagnosis and decision making of this personalized treatment plan is one of the current developments in precision medicine [3, 4]. smart healthcare aims to provide intelligent comprehensive differentiation and prescription recommendation for the diagnosis and treatment of diseases  by applying artificial intelligence technology and cloud computing to the practice of clinical medicine. it has been greatly developed through the applications of artificial intelligence, cloud computing, big data analysis, and internet of things (iot), and has been applied to many medical fields such as intelligent chinese medicine and intelligent testing. medical big data is to integrate the iot system into medicine and to integrate and classify the collected medical data information by creating the medical internet of things [5, 6]. the deep learning model and the deep reinforcement learning model are the most commonly used artificial intelligence models, which can be trained and simulated by providing a large number of training examples through medical big data. the computer aids of modern medicine and traditional chinese medicine have matured. thus, there are many well-trained deep learning models for clinical medicine. regarding sgs for detecting of pulmonary pathogen infection, multiple agents and multitask are promising. therefore, the deep reinforcement learning based on multitask transfer learning is a feasible direction. deep reinforcement learning based on memory and reasoning. the traditional visual-perception-based drl method is far worse than human beings in solving higher-level cognition-inspired tasks. that is to say, in solving some high-level drl tasks, the agent not only needs strong perceptual ability, but also needs certain memory and reasoning ability to learn effective decision-making. therefore, the ability to give active learning and reasoning to existing drl models is very important. all of these approaches require features with which the input sequence can be classified. the features used for classification are mainly k-mer based on various k sizes between 1-8. in the case of probabilistic models and similarity rankings, not only the viral genomes but also the host genomes have to be analyzed.in this study, we presented the tool vidhop and investigated the usability of deep learning for the prediction of hosts for distinct viruses, based on the viral nucleotide sequences alone. we established a simple but very capable prediction pipeline, including possible data preparation steps, data training strategies, and a suitable deep neural network architecture. besides, we provide three different neural network models, which can predict potential hosts for either influenza a  , rotavirus a, or rabies  lyssavirus, respectively. these deep neural networks are used in vidhop and use genomic fragments shorter than 400 nucleotides to predict potential virus hosts directly on a species level. in contrast to similar approaches, this is a more complex task than performing host prediction only on the genera level (zhang et al., 2017; galiez et al., 2017) or even higher taxonomic groups. moreover, our approach can predict more hosts with comparable accuracy than previous approaches. the consistently high average accuracy of vidhop, on all three datasets, indicates the versatility of the deep learning approach we used. additionally, we addressed multiple problems that arise when using dna or rna sequences as input for deep learning, such as unbalanced datasets for training and the problem of inefficient learning of recurrent neural networks (rnn) on long sequences. we evaluated different solutions to solve these problems and observed that splitting of the original virus genome sequence in combination with merging the prediction results of the generated subsequences leads to fast and efficient learning on long sequences. furthermore, the use of unbalanced datasets is possible if a new balanced training set is generated by repeated random undersampling (a random selection of available sequences) for every single epoch during the training phase. most previous research has attempted to predict infectious disease  using internet search query data alone. however, as discussed above, it is necessary to also consider various big data and environmental factors such as weather when predicting infectious disease. in addition, in the case of models that use deep learning, it is possible to improve prediction performance by optimizing the deep learning model by optimizing its parameters. therefore, the aim of this study is to design a model that uses the infectious disease  occurrence data provided by the kcdc, search query data from search engines that are specialized for south korea, twitter social media big data, and weather data such as temperature and humidity. according to a study by kwon et al., a model that considers the time difference between clinical and non-clinical data can detect infectious disease  outbreaks one to two weeks before current surveillance systems [43]. therefore, this study adds lag to the collected dataset to take temporal characteristics into account. in addition, in the design process, a thorough testing of all the input variable combinations is performed to examine the effects of each resulting dataset on infectious disease  outbreaks and select the optimal model with the most explanatory power. the model’s prediction performance is verified by comparing it with an infectious disease  prediction model that uses a deep learning method and an infectious disease  prediction model that uses time series analysis. predicting infectious disease  using deep learning and big data infectious disease  occurs when a person is infected by a pathogen from another person or an animal. it not only harms individuals, but also causes harm on a macro scale and, therefore, is regarded as a social problem [1]. at the korea center for disease control (kcdc), infectious disease  surveillance is a comprehensive process in which information on infectious disease  outbreaks and vectors are continuously and systematically collected, analyzed, and interpreted. moreover, the results are distributed quickly to people who need them to prevent and control infectious disease. the kcdc operates a mandatory surveillance system in which mandatory reports are made without delay to the relevant health center when an infectious disease  occurs and it operates a sentinel surveillance system in which the medical organization that has been designated as the sentinel reports to the relevant health center within seven days. the targets of mandatory surveillance consist of a total of 59 infectious diseases from groups 1 to 4 by the kcdc. the targets of sentinel surveillance include influenza  from group 3 along with 21 infectious diseases  from group 5. overall, a total of 80 infectious diseases in six groups are monitored. in the current korean infectious disease reporting system, if there is a legally defined infectious disease  patient at a medical organization, a report is made to the managing health center through the infectious disease  web reporting system. the managing health center reports to the city and province health offices through another system and the city and province health offices report to the kcdc. lstm is the core of attention deep learning algorithm. it can learn the features of the data far apart in the text data, which provides support for learning the relationship between physiological parameters mentioned above, and improves the performance of the auxiliary diagnostic model. the purpose of lstm is to study the joint representation of different physiological parameters. in clinical practice, disease-related physiological parameters are not independent, so lstm is more suitable for analyzing textual medical data with joint characteristics than traditional methods. the schematic diagram of lstm layer is shown in figure 3. another challenge in applying deep learning algorithms to auxiliary diagnosis is the interpretability. up to now, the deep learning model is still a black-box model, which cannot explain exactly which kind of physiological parameters plays a vital role in the process of data processing. the development of disease diagnosis technology depends not only on the improvement of diagnostic accuracy but also on the discovery of more effective diagnostic markers and the relationship between different physiological parameters ( diseases  ). it is difficult to meet the requirements above by the auxiliary diagnosis system which only gives the diagnosis results. as we know, the human brain tends to have an attention focus when processing things, and it is able to find out important features purposefully according to the environment, this mechanism is called the attention mechanism. the combination of attention mechanism and deep learning model can imitate the functions of the human brain mentioned above, which has been proved to have the ability to focus on important features and has been applied in many fields such as image recognition and semantic recognition.23–25 therefore, in order to solve the above problems, this paper not only studies the automatic diagnosis of diseases  with human physiological parameters, but also applies the attention mechanism to the auxiliary diagnosis model. this method gives the importance of different physiological parameters for disease diagnosis while automatically diagnosing diseases  , enhances the interpretability of the model, and further enhances the assistant ability of the auxiliary diagnosis system for clinical research. this algorithm is called attention deep learning. there was significant difference in temperature in the regions corresponding to each foot whenever there is an abnormality. registration is approximate in the sense that when the big toe is amputated, the bounding box heights differ and hence the regions are not properly segmented. the hot and cold regions extracted from the images suggest the location of an underlying inflammation or the extent of inflammation if present already in the case of ulcer  or the presence of neuropathy. the isolation of these regions will help the doctor in identifying the underlying angiosome for assisting in therapy or surgery. the dataset used is small and it has to be built with more images so that the analysis is more accurate. the next step is to build such a dataset. segmentation of the foot from the background posed problems in the case of diabetic patients when the toes were cold. hence we are looking forward to build a deep learning network to do the segmentation. lack of medical and bio-thermal knowledge has to be covered by discussion with medical experts. d. reproducibility of the images is a great concern due to the large set of factors involved. e. takes long time for the entire process from thermal image acquisition stage to the classification stage as it required repeated capturing of images in particular time gaps. f. identifying the best deep learning architecture for good accuracy in classification. one important factor when it comes to image processing these days which is not included in the previous set of factors is the hardware requirements. with the advancement of technology and software computing capabilities, there is a compelling need to improve the features of the underlying hardware to support high dimensionality as is the case of deep learning systems. this is becoming a basic requirement in the case of image processing using deep learning architectures. graphical processing units (gpu) with parallel architectures are the way forward for deep learning systems built to analyze thermal images. since deep learning is very computationally intensive, we will need a fast cpu with many cores. deep learning techniques, such as deep boltzmann machines (dbms), have received considerable attention over the past years due to the outstanding results concerning a variable range of domains. one of the main shortcomings of these techniques involves the choice of their hyperparameters, since they have a significant impact on the final results. this work addresses the issue of fine-tuning hyperparameters of deep boltzmann machines using metaheuristic optimization techniques with different backgrounds, such as swarm intelligence, memory- and evolutionary-based approaches. experiments conducted in three public datasets for binary image reconstruction showed that metaheuristic techniques can obtain reasonable results. we propose a new framework, translation between augmented natural languages (tanl), to solve many structured prediction language tasks including joint entity and relation extraction, nested named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, and dialogue state tracking. instead of tackling the problem by training task-specific discriminative classifiers, we frame it as a translation task between augmented natural languages, from which the task-relevant information can be easily extracted. our approach can match or outperform task-specific models on all tasks, and in particular, achieves new state-of-the-art results on joint entity and relation extraction (conll04, ade, nyt, and ace2005 datasets), relation classification (fewrel and tacred), and semantic role labeling (conll-2005 and conll2012). we accomplish this while using the same architecture and hyperparameters for all tasks and even when training a single model to solve all tasks at the same time (multi-task learning). finally, we show that our framework can also significantly improve the performance in a low-resource regime, thanks to better use of label semantics. abstract—breast cancer is among the most deadly diseases, distressing mostly women worldwide. although traditional methods for detection have presented themselves as valid for the task, they still commonly present low accuracies and demand considerable time and effort from professionals. therefore, a computer-aided diagnosis (cad) system capable of providing early detection becomes hugely desirable. in the last decade, machine learningbased techniques have been of paramount importance in this context, since they are capable of extracting essential information from data and reasoning about it. however, such approaches still suffer from imbalanced data, specifically on medical issues, where the number of healthy people samples is, in general, considerably higher than the number of patients. therefore this paper proposes the o2pf, a data oversampling method based on the unsupervised optimum-path forest algorithm. experiments conducted over the full oversampling scenario state the robustness of the model, which is compared against three well-established oversampling methods considering three breast cancer and three general-purpose tasks for medical issues datasets. in this concept paper, we discuss intricacies of specifying and verifying the quality of continuous and lifelong learning artificial intelligence systems as they interact with and influence their environment causing a so-called concept drift. we signify a problem of implicit feedback loops, demonstrate how they intervene with user behavior on an exemplary housing prices prediction system. based on a preliminary model, we highlight conditions when such feedback loops arise and discuss possible solution approaches. deep learning has become the gold standard for image processing over the past decade. simultaneously, we have seen growing interest in orbital activities such as satellite servicing and debris removal that depend on proximity operations between spacecraft. however, two key challenges currently pose a major barrier to the use of deep learning for vision-based onorbit proximity operations. firstly, efficient implementation of these techniques relies on an effective system for model development that streamlines data curation, training, and evaluation. secondly, a scarcity of labeled training data (images of a target spacecraft) hinders creation of robust deep learning models. this paper presents an open-source deep learning pipeline, developed specifically for on-orbit visual navigation applications, that addresses these challenges. the core of our work consists of two custom software tools built on top of a cloud architecture that interconnects all stages of the model development process. the first tool leverages blender, an open-source 3d graphics toolset, to generate labeled synthetic training data with configurable model poses (positions and orientations), lighting conditions, backgrounds, and commonly observed in-space image aberrations. the second tool is a plugin-based framework for effective dataset curation and model training; it provides common functionality like metadata generation and remote storage access to all projects while giving complete independence to project-specific code. time-consuming, graphics-intensive processes such as synthetic image generation and model training run on cloud-based computational resources which scale to any scope and budget and allow development of even the largest datasets and models from any machine. the presented system has been used in the texas spacecraft laboratory with marked benefits in development speed and quality. remote development, scalable compute, and automatic organization of data and artifacts have dramatically decreased iteration time while increasing reproducibility and system comprehension. diverse, high-fidelity synthetic images that more closely replicate the real environment have improved model performance against real-world data. these results demonstrate that the presented pipeline offers tangible benefits to the application of deep learning for vision-based on-orbit proximity operations. parkinson’s disease (pd) is a chronic, degenerative disorder which leads to a range of motor and cognitive symptoms. pd diagnosis is a challenging task since its symptoms are very similar to other diseases such as normal ageing and essential tremor. much research has been applied to diagnosing this disease. this project aims to automate the pd diagnosis process using deep learning, recursive neural networks (rnn) and convolutional neural networks (cnn), to differentiate between healthy and pd patients. besides that, since different datasets may capture different aspects of this disease, this project aims to explore which pd test is more effective in the discrimination process by analysing different imaging and movement datasets (notably cube and spiral pentagon datasets). in addition, this project evaluates which dataset type, imaging or time series, is more effective in diagnosing pd. abstract—as gradient descent method in deep learning causes a series of questions, this paper proposes a novel gradient-free deep learning structure. by adding a new module into traditional self-organizing map and introducing residual into the map, a deep valued self-organizing map network is constructed. and analysis about the convergence performance of such a deep valued self-organizing map network is proved in this paper, which gives an inequality about the designed parameters with the dimension of inputs and the loss of prediction. deep ensembles perform better than a single network thanks to the diversity among their members. recent approaches regularize predictions to increase diversity; however, they also drastically decrease individual members’ performances. in this paper, we argue that learning strategies for deep ensembles need to tackle the trade-off between ensemble diversity and individual accuracies. motivated by arguments from information theory and leveraging recent advances in neural estimation of conditional mutual information, we introduce a novel training criterion called dice: it increases diversity by reducing spurious correlations among features. the main idea is that features extracted from pairs of members should only share information useful for target class prediction without being conditionally redundant. therefore, besides the classification loss with information bottleneck, we adversarially prevent features from being conditionally predictable from each other. we manage to reduce simultaneous errors while protecting class information. we obtain state-of-the-art accuracy results on cifar-10/100: for example, an ensemble of 5 networks trained with dice matches an ensemble of 7 networks trained independently. we further analyze the consequences on calibration, uncertainty estimation, out-of-distribution detection and online co-distillation. in order for agents trained by deep reinforcement learning to work alongside humans in realistic settings, we will need to ensure that the agents are robust. since the real world is very diverse, and human behavior often changes in response to agent deployment, the agent will likely encounter novel situations that have never been seen during training. this results in an evaluation challenge: if we cannot rely on the average training or validation reward as a metric, then how can we effectively evaluate robustness? we take inspiration from the practice of unit testing in software engineering. specifically, we suggest that when designing ai agents that collaborate with humans, designers should search for potential edge cases in possible partner behavior and possible states encountered, and write tests which check that the behavior of the agent in these edge cases is reasonable. we apply this methodology to build a suite of unit tests for the overcooked-ai environment, and use this test suite to evaluate three proposals for improving robustness. we find that the test suite provides significant insight into the effects of these proposals that were generally not revealed by looking solely at the average validation reward. the input space of a neural network with relu-like activations is partitioned into multiple linear regions, each corresponding to a specific activation pattern of the included relu-like activations. we demonstrate that this partition exhibits the following encoding properties across a variety of deep learning models: (1) determinism: almost every linear region contains at most one training example. we can therefore represent almost every training example by a unique activation pattern, which is parameterized by a neural code; and (2) categorization: according to the neural code, simple algorithms, such as k-means, k-nn, and logistic regression, can achieve fairly good performance on both training and test data. these encoding properties surprisingly suggest that normal neural networks well-trained for classification behave as hash encoders without any extra efforts. in addition, the encoding properties exhibit variability in different scenarios. further experiments demonstrate that model size, training time, training sample size, regularization, and label noise contribute in shaping the encoding properties, while the impacts of the first three are dominant. we then define an activation hash phase chart to represent the space expanded by model size, training time, training sample size, and the encoding properties, which is divided into three canonical regions: under-expressive regime, critically-expressive regime, and sufficiently-expressive regime. electroencephalograph (eeg) emotion recognition is a significant task in the brain-computer interface field. although many deep learning methods are proposed recently, it is still challenging to make full use of the information contained in different domains of eeg signals. in this paper, we present a novel method, called four-dimensional attention-based neural network (4d-ann) for eeg emotion recognition. first, raw eeg signals are transformed into 4d spatial-spectral-temporal representations. then, the proposed 4d-ann adopts spectral and spatial attention mechanisms to adaptively assign the weights of different brain regions and frequency bands, and a convolutional neural network (cnn) is utilized to deal with the spectral and spatial information of the 4d representations. moreover, a temporal attention mechanism is integrated into a bidirectional long short-term memory (lstm) to explore temporal dependencies of the 4d representations. our model achieves state-of-the-art performance on the seed dataset under intra-subject splitting. the experimental results have shown the effectiveness of the attention mechanisms in different domains for eeg emotion recognition. adam is one of the most influential adaptive stochastic algorithms for training deep neural networks, which has been pointed out to be divergent even in the simple convex setting via a few simple counterexamples. many attempts, such as decreasing an adaptive learning rate, adopting a big batch size, incorporating a temporal decorrelation technique, seeking an analogous surrogate, etc., have been tried to promote adam-type algorithms to converge. in contrast with existing approaches, we introduce an alternative easy-to-check sufficient condition, which merely depends on the parameters of the base learning rate and combinations of historical second-order moments, to guarantee the global convergence of generic adam for solving large-scale non-convex stochastic optimization. this observation coupled with this sufficient condition gives much deeper interpretations on the divergence of adam. on the other hand, in practice, mini-adam and distributed-adam are widely used without theoretical guarantee, we further give an analysis on how will the batch size or the number of nodes in the distributed system will affect the convergence of adam, which theoretically shows that mini-batch and distributed adam can be linearly accelerated by using a larger mini-batch size or more number of nodes. at last, we apply the generic adam and minibatch adam with sufficient condition for solving the counterexample and training several different neural networks on various real-world datasets. experimental results are exactly in accord with our theoretical analysis. the drastic increase of data quantity often brings the severe decrease of data quality, such as incorrect label annotations, which poses a great challenge for robustly training deep neural networks (dnns). existing learning methods with label noise either employ ad-hoc heuristics or restrict to specific noise assumptions. however, more general situations, such as instance-dependent label noise, have not been fully explored, as scarce studies focus on their label corruption process. by categorizing instances into confusing and unconfusing instances, this paper proposes a simple yet universal probabilistic model, which explicitly relates noisy labels to their instances. the resultant model can be realized by dnns, where the training procedure is accomplished by employing an alternating optimization algorithm. experiments on datasets with both synthetic and real-world label noise verify that the proposed method yields significant improvements on robustness over state-of-the-art counterparts. integer quantization of neural networks can be defined as the approximation of the high precision computation of the canonical neural network formulation, using reduced integer precision. it plays a significant role in the efficient deployment and execution of machine learning (ml) systems, reducing memory consumption and leveraging typically faster computations. in this work, we present an integer-only quantization strategy for long short-term memory (lstm) neural network topologies, which themselves are the foundation of many production ml systems. our quantization strategy is accurate (e.g. works well with quantization post-training), efficient and fast to execute (utilizing 8 bit integer weights and mostly 8 bit activations), and is able to target a variety of hardware (by leveraging instructions sets available in common cpu architectures, as well as available neural accelerators). by incorporating structured pairs of non-trainable input and output layers, the universal approximation property of feed-forward have recently been extended across a broad range of non-euclidean input spaces x and output spaces y. we quantify the number of narrow layers required for these ”deep geometric feed-forward neural networks” (dgns) to approximate any continuous function in c(x, y ), uniformly on compacts. the dgn architecture is then extended to accommodate complete riemannian manifolds, where the input and output layers are only defined locally, and we obtain local analogs of our results. in this case, we find that both the global and local universal approximation guarantees can only coincide when approximating null-homotopic functions. consequentially, we show that if y is a compact riemannian manifold, then there exists a function that cannot be uniformly approximated on large compact subsets of x. nevertheless, we obtain lower-bounds of the maximum diameter of any geodesic ball in x wherein our local universal approximation results hold. applying our results, we build universal approximators between spaces of non-degenerate gaussian measures. we also obtain a quantitative version of the universal approximation theorem for classical deep narrow feed-forward networks with general activation functions. scientifically evaluating soccer players represents a challenging machine learning problem. unfortunately, most existing answers have very opaque algorithm training procedures; relevant data are scarcely accessible and almost impossible to generate. in this paper, we will introduce a two-part solution: an open-source player tracking model and a new approach to evaluate these players based solely on deep reinforcement learning, without human data training nor guidance. our tracking model was trained in a supervised fashion on datasets we will also release, and our evaluation model relies only on simulations of virtual soccer games. combining those two architectures allows one to evaluate soccer players directly from a live camera without large datasets constraints. we term our new approach expected discounted goal (edg), as it represents the number of goals a team can score or concede from a particular state. this approach leads to more meaningful results than the existing ones that are based on real-world data, and could easily be extended to other sports. anomaly detection systems need to consider a lot of information when scanning for anomalies. one example is the context of the process in which an anomaly might occur, because anomalies for one process might not be anomalies for a different one. therefore data – such as system events – need to be assigned to the program they originate from. this paper investigates whether it is possible to infer from a list of system events the program whose behavior caused the occurrence of these system events. to that end, we model transition probabilities between nonequivalent events and apply the k-nearest neighbors algorithm. this system is evaluated on nonmalicious, real-world data using four different evaluation scores. our results suggest that the approach proposed in this paper is capable of correctly inferring program names from system events. —deep learning plays a significant role in assisting humans in many aspects of their lives. as these networks tend to get deeper over time, they extract more features to increase accuracy at the cost of additional inference latency. this accuracyperformance trade-off makes it more challenging for embedded systems, as resource-constrained processors with strict deadlines, to deploy them efficiently. this can lead to selection of networks that can prematurely meet a specified deadline with excess slack time that could have potentially contributed to increased accuracy. in this work, we propose: (i) the concept of layer removal as a means of constructing trimmed networks (trns) that are based on removing problem-specific features of a pretrained network used in transfer learning, and (ii) netcut, a methodology based on an empirical or an analytical latency estimator, which only proposes and retrains trns that can meet the application’s deadline, hence reducing the exploration time significantly. we demonstrate that trns can expand the pareto frontier that trades off latency and accuracy to provide networks that can meet arbitrary deadlines with potential accuracy improvement over off-the-shelf networks. our experimental results show that such utilization of trns, while transferring to a simpler dataset, in combination with netcut, can lead to the proposal of networks that can achieve relative accuracy improvement of up to 10.43% among existing off-the-shelf neural architectures while meeting a specific deadline, and 27x speedup in exploration time. lime: learning inductive bias for primitives of mathematical reasoning yuhuai wu1 , markus rabe2 , wenda li3 , jimmy ba1 , roger grosse1 , and christian szegedy2 1university of toronto, vector institute 2google 3university of cambridge {ywu,jba,rgrosse}@cs.toronto.edu, {mrabe, szegedy}@google.com, wl302@cam.ac.uk abstract while designing inductive bias in neural architectures has been widely studied, we hypothesize that transformer networks are flexible enough to learn inductive bias from suitable generic tasks. here, we replace architecture engineering by encoding inductive bias in the form of datasets. inspired by peirce’s view that deduction, induction, and abduction form an irreducible set of reasoning primitives, we design three synthetic tasks that are intended to require the model to have these three abilities. we specifically design these synthetic tasks in a way that they are devoid of mathematical knowledge to ensure that only the fundamental reasoning biases can be learned from these tasks. this defines a new pre-training methodology called “lime” (learning inductive bias for mathematical reasoning). models trained with lime significantly outperform vanilla transformers on three very different large mathematical reasoning benchmarks. unlike dominating the computation cost as traditional pre-training approaches, lime requires only a small fraction of the computation cost of the typical downstream task. 1 introduction inductive bias is essential for successful neural network learning. many of the breakthroughs in machine learning are accompanied by new neural architectures with better inductive biases, such as locality bias in convolutional neural networks (lecun et al., 1999), recurrence and memory in lstms (hochreiter and schmidhuber, 1997), and structural bias in graph neural networks (scarselli et al., 2008). however, existing designs of inductive biases need to be explicitly encoded in neural architecture. this is sometimes difficult as one may not know the exact mechanism for an abstract ability, in order to describe the architectural bias explicitly. in particular, designing proper inductive bias for abstract concepts such as mathematical reasoning becomes an extremely challenging task. moreover, attempts to design elaborate architectures for reasoning often fall short of the performance of more generic transformer architecture. in this work, we aim to avoid the search for new architectures and investigate whether one can learn useful inductive bias for mathematical reasoning through pretraining. large-scale unsupervised pretraining of language models revolutionized the field of natural language processing (nlp), improving the state-of-the-art in question answering, name entity recognition, text classification, and other domains, e.g. (radford et al., 2018; devlin et al., 2019; yang et al., 2019; liu et al., 2019; raffel et al., 2020; brown et al., 2020). as a result, pretraining has become a common practice for modern neural network based nlp. one plausible explanation for the benefit of pretraining is that the model can learn world knowledge by memorizing the contents of the natural language corpus. this can be useful in various natural language downstream tasks, such as question answering and text classification. however, there is another potential advantage of pre-training—it may distill inductive biases into the model that are helpful for training on downstream tasks (brown et al., 2020; warstadt and bowman, 2020). we focus on the latter and design pre-training tasks that are intentionally devoid of knowledge and only allow the model to learn inductive bias for reasoning. 1 arxiv:2101.06223v1 [cs.lg] 15 jan 2021 inspired by the logician charles peirce (peirce, 1992), we believe that the following three primitives are the most crucial for reasoning: 1. deduction: the ability to deduce new truths from given facts and inference rules. 2. induction: the ability to induce general inference rules from a set of known facts. 3. abduction: the ability to explain the relationship between the evidences and inference rules. to endow the models with an inductive bias for mathematical reasoning, we design a synthetic task for each of the three inductive biases. we hypothesize that the transformer networks are flexible enough to learn strong inductive bias from the three synthetic reasoning tasks and consequently improving the downstream tasks. although such inductive bias may be useful in general reasoning tasks (e.g., nlp tasks), in this work, we focus on mathematical reasoning benchmarks, for which we expect to observe the largest gains. we call training on these tasks lime – an acronym for “learning inductive bias for mathematical reasoning”. note that there is only a limited amount of pretraining data available for formal mathematical benchmarks, therefore the study of generic pre-training techniques is particularly important for the success of machine learning in mathematical reasoning. we demonstrate that lime pretrained models provide significant gains across three large mathematical reasoning benchmarks: isarstep (li et al., 2020), holist skip-tree (rabe et al., 2020) and metamathstep (polu and sutskever, 2020). notably, on the isarstep benchmark, pre-training improved the top-1 accuracy from 20.4% to 26.9% and top-10 accuracy from 33.1% to 41.0%. compared to the traditional pre-training tasks, there are two major differences. first, we do not load the input embeddings or the weights in the output layer for finetuning on downstream tasks. this allows us to use the same pre-trained model for a variety of downstream tasks, which can have vastly different vocabularies due to language or tokenization differences. also, it prevents the transfer of content knowledge from the pretraining to downstream tasks, supporting the evidence of learning inductive biases. furthermore, pretraining on synthetic tasks require only a fraction of the computational cost of downstream tasks. with only about two hours of training on a single modern gpu, one already obtains all the benefits, in contrast to days of training on a large natural language corpus with hundreds of gpus/tpus. our method can also be regarded as a form of curriculum learning, in which the model is taught basic, extremely generic but general skills before being trained on the specific problem domain. to summarize, the contributions of the paper are: 1. providing the first method to design inductive biases in the form of datasets for mathematical reasoning. 2. demonstrating significant improvements in the reasoning performance of transformer models on three large mathematical reasoning benchmarks with negligible extra computation cost. 3. by showing how pretraining brings benefits other than learning content knowledge, disentangling the study of its working mechanism. 2 related work learning models applied to mathematics there has been increasing interest in applying deep learning methods to interactive theorem provers (itp) (bansal et al.; 2019; gauthier et al., 2020; huang et al., 2019; yang and deng, 2019; wu et al., 2020; li et al., 2020; polu and sutskever, 2020). the work that is most related to ours is gpt-f (polu and sutskever, 2020). the authors performed pretraining on several natural language corpora and showed significant improvements for an itp system – metamath. different from ours, they used gpt-style large-scale language modeling pretraining, which dominates the computation cost compared to the downstream task. we, on the other hand, propose pretraining on a few lightweight synthetic tasks costing only a minor fraction of the computation spent on the downstream task. lample and charton (2020) have demonstrated that transformer models can be used for symbolic mathematics by successfully predicting the integrals of formulas from a randomly generated dataset. similar observations are made for logical problems relevant to verification: that transformer networks can learn the semantics of logics (hahn et al., 2020). rabe et al. (2020) have shown that mathematical reasoning can emerge from self-supervised training alone. li et al. (2020) show that language models 2 can learn to synthesize missing high-level intermediate propositions given a local context. piotrowski and urban (2020) used rnns in automated theorem provers for first-order logic. wang et al. (2020) explored the use of machine translation to translate between synthetically generated natural language descriptions of proofs and formally represented proofs. urban and jakub˚uv (2020) present initial experiments on generating mathematical conjectures with a transformer model. saxton et al. (2019) suggest a dataset for the analysis of mathematical reasoning skills. in contrast to the datasets considered here, their dataset is synthetic, focuses on calculation with concrete numbers, and only contains relatively few symbolic tasks. language model pretraining the advent of the transformer architecture (vaswani et al., 2017) and the bert style pretraining (devlin et al., 2019) represented a huge improvement in the quality of language modeling. since then, an explosion of research activity in the area pushed the quality of language models through better pretraining tasks. where bert (devlin et al., 2019) masks out a fraction of the input tokens, later works demonstrated the advantages of masking out subsequences (song et al., 2019; dong et al., 2019; joshi et al., 2020; raffel et al., 2020; conneau and lample, 2019) and whole sentences (zhang et al., 2020). besides the choice of pretraining tasks, the scale of language models is also an important factor. language models improve in quality and develop new abilities as they grow larger while trained on the same data (radford et al., 2018; raffel et al., 2020; brown et al., 2020). inductive biases in general there have been works studying learning inductive biases in other contexts. in particular, mccoy et al. (2020) studied whether one can learn linguistic inductive biases on synthetic datasets via meta-learning. papadimitriou and jurafsky (2020) shows inductive biases learned in music data can be useful for natural language. they further designed several synthetic tasks and showed similar kind of improvements for natural language tasks. from a more theoretical point of view, xu et al. (2020) formalize an aspect of inductive (architectural) bias under the context of gnns, with a notation called architectural alignment. the architecture is aligned when the architecture can perfectly simulates the ground truth solution. but their work is limited to showing alignment in combinatorial problems, whose ground truth solutions are known. in contrast, our work tries to learn architectural bias by relying on the flexible transformer architecture and training on synthetic datasets. inductive biases for mathematics previous work studying inductive biases for logical reasoning has focused on encoding bias in the neural architecture. initial works focused on encoding the tree structure of expressions using treernns (evans et al., 2018). graph neural networks are shown to provide a much stronger performance than tree models in premise selection (wang et al., 2017) and theorem proving (paliwal et al., 2020). gnns also scale to larger formulas in sat (selsam et al., 2019; selsam and bjørner, 2019; han, 2020), qbf (lederman et al., 2020), and #sat (vaezipoor et al., 2020). crouse et al. (2019) have shown that pooling mechanisms can have an impact on the performance of gnns on logical formulas as well. closely related, hellendoorn et al. (2020) have shown that it can be helpful to hard-code the tree structure of programs in the attention mask of transformers. schlag et al. (2019) developed an architecture for encoding relational information using tensor product representation for mathematical reasoning. 3 methods in this section, we first discuss the primitives of reasoning, inspired by peirce’s views, and design one synthetic task for each reasoning primitive. 3.1 reasoning primitives in peirce’s view, there are exactly three kinds of reasoning: deduction, abduction, and induction. deduction is known as the workhorse for mathematics. it is the process of deriving new facts by applying logical inference rules to known facts or premises. on the other hand, abduction and induction can be thought of as the inverses of deduction. if we call the premise used in deduction as case, its logical rule as rule, and its conclusion as result, then abduction is equivalently the inference of a case from a rule and a result, while induction may be said to be the inference of a rule from a case and a result. we summarize the three reasoning primitives in the following table: 3 reasoning primitives inference map deduction rule, case → result abduction rule, result → case induction case, result → rule to give an example, we let rule be “all the beans in this bag are white”, case be “these beans are from this bag”, and result be “these beans are white”. deduction is to derive the fact that these beans are white (re) from knowing all the beans from this bag are white (r) and these beans are from this bag (c). abduction explains why the beans are white (re) from knowing that all the beans in the bag are white (r) – because these beans must be from the bag (c). lastly, induction aims to provide a general principle to observing the fact that the beans are white (re) and they come from this bag (c), which is that all the beans in the bag must be white (r). we refer to peirce (1992) and bellucci and pietarinen (2015) for more elaborate discussions on the primitives of reasoning. mathematical reasoning exhibits nontrivial uses of these reasoning primitives. deduction happens when one needs to derive new valid statements from the given premise (case) and theorems in the library (rule). abduction is used to postulate conjectures from the known facts and theorems, allowing one to decompose the challenging theorem into subgoals for proof. induction, the ability to extract general principles from known facts and theorems is also one of the major activities of mathematical reasoning. it is used when one derives theorems from special cases and proposes new definitions and general frameworks to encapsulate existing knowledge. 3.2 lime synthetic tasks for reasoning primitives we design three synthetic tasks inspired by the three reasoning primitives. as discussed in the previous section, all of the reasoning primitives consist of three essential elements: rule, case, and result. inspired by this, we first design a method to generate those elements. once they are generated, we can construct tasks that predict one element from the other two. in the following, we describe one simple way to generate those three elements, though we acknowledge that there are many other possible approaches. we require two types of symbols: 1. math symbols, 2. rule symbols. in general, these symbols can take any forms (e.g., integer representations). but for the ease of discussion, we will think of math symbols as the union of those operators used in mathematics (e.g., “+ − ∗ = ()&”) and lower case letters (e.g., a, b, c . . . ), and rule symbols as upper case letters (e.g., a, b, c . . . ). we now construct rule, case, and result in order: 1. rule is a randomly sampled string that consists of i) rule symbols and ii) math symbols. the length of the string is randomly sampled from a range. for instance, a randomly sampled rule can be: a ∗ a + b = c with rule symbols a, b, and c. 2. case is a dictionary that represents substitutions. for each rule symbol used in the rule string, we sample a random string of random length that consists of math symbols. this forms a dictionary, whose keys are all rule symbols, and the values are the corresponding sampled string. to illustrate, following the previous example, for each a, b and c, we sample a random string to form a dictionary as: {a : a, b : b, c : d + e}. 3. result is the outcome of the substitution. for each rule symbol in the rule string, we replace it with the corresponding value stored in the case dictionary. this gives rise to the result string. as per the previous example, we now substitute a with a, b with b, and c with d + e into the rule string, generating the result string: a ∗ a + b = d + e. after rule, case, and result are generated, we can construct three tasks for deduction, abduction, and induction respectively. we define the three synthetic tasks as follows: • deduct: source: rule string and case dictionary. target: result string. • abduct: source: rule string and result string. target: case dictionary. • induct: source: case dictionary and result string. target: rule string. we also consider a task called mix, which is a uniform mix of three tasks. namely, during generation, we randomly select a task and sample an example from that task. to formulate them as sequence to sequence tasks, we represent the case dictionary also as a string, e.g., “{a : a, b : b, c : d + e}”. 4 an example of abduct using the examples of rule, case, and result above is to predict the target {a : a, b : b, c : d + e} from the source a ∗ a + b = c a ∗ a + b = d + e. pre-training on our synthetic tasks can be seen as a form of skip-component learning. there are three essential components: rule, case and result, and we skip one of them and use the remaining two elements to reconstruct the missing one. past work has shown that learning to predict missing words (devlin et al., 2019), subsequences (song et al., 2019; raffel et al., 2020), or subtrees (rabe et al., 2020) are strong pre-training tasks. 3.3 symbol-agnostic representation in order to solve the synthetic tasks, the model needs to distinguish which set of symbols can be substituted (rule symbols). as a result, the model may memorize information about the symbols that is irrelevant to the inductive biases encoded in the task. to prevent such memorization, we propose a way to make the synthetic tasks agnostic to the choice of symbols. we first note that the choice of symbols is irrelevant to our synthetic tasks. to avoid symbol-specific memorization, for each training and evaluation example, we randomly sample two sets of symbols to be used in rules and in the rest of the example. but for the abduct task, the model needs to know which symbols are replaced by the rule part of the example and which symbols are in the result language. we simply list the split of the symbols used in the example at the beginning of the input string, marked by two special symbols, and . they are followed by the original source string. the target string remains unchanged. for example, the previous example in the abduct task becomes, source: a b c ∗ + = a b d e a ∗ a + b = c a ∗ a + b = d + e target: {a : a, b : b, c : d + e} in our implementation, we use integers to represent symbols. specifically, for each example, we sample two disjoint sets of integers from the set {1, . . . , s} to represent the math symbols and the rule symbols, where s is the size of the vocabulary. in our experiments, we sample 44 math symbols and 24 rule symbols for each problem. the complete pseudo-code of generating the symbols, rule, case, and result for one task example is provided in appendix algorithm 1. 4 experiments in this section, we present results on three large mathematical reasoning tasks that are especially useful in the context of automated theorem proving. our results show significant gains in learning inductive biases from synthetic tasks. we have selected three tasks to cover three different styles of interactive theorem provers: the hol-light (skip-tree) corpus was created from very high-level tactic-based proofs, but it is less interpretable than isarstep’s declarative style corpus. we also evaluate the next proof-step prediction task on the set.mm library of metamath, which consists of very granular, basic proof steps. namely, the proof steps are more predicable and average proof lengths have significantly increased. 4.1 experiment details lime pretraining we generate datasets of our synthetic tasks for pretraining: deduct, abduct, induct, mix. for pretraining of isarstep, we used a vocabulary size s of 1000. for the other two downstream tasks, we used a vocabulary size of 100. the reason we used different vocabulary sizes was that we found (cf. appendix) the discrepancy in vocabulary size affects the performance of a downstream task if it has a very large vocabulary size (isarstep has 28k). we use 44 math symbols and 24 rule symbols. the length of the rule string is sampled from 5 to 20, the length of the string for each substitution (the values of case dictionary) is sampled from 2 to 8. we used word-level tokenization for all the tasks. we pretrained the model for 20k updates. for tasks with larger vocabulary size (i.e., 1000), we found the learning became more difficult. hence we used a curriculum learning scheme: we first trained the model for 10k steps on the same task with a vocabulary size of 100, then continue training for another 10k step on vocabulary size of 1000. the pretraining was done on a single nvidia tesla t4 gpu with 4 cpu cores for 2 hours. we set the 5 table 1: test top-1, top-10 (%) accuracy on the isarstep task. model top-1 acc. top-10 acc. no pretrain (li et al., 2020) 20.4 33.1 hat (li et al., 2020) 22.8 35.2 lime deduct 24.7 37.7 lime abduct 26.7 41.0 lime induct 23.9 38.8 lime mix 26.9 40.4 table 2: test top-8 accuracy on skip-tree holist (%). model equation completion hard type inference missing assumptions easy type inference no pretrain (rabe et al., 2020) 46.3 95.0 41.8 95.9 lime deduct 50.3 94.8 47.9 97.0 lime abduct 48.4 94.8 46.1 96.3 lime induct 44.8 94.9 42.6 96.4 lime mix 51.7 95.6 46.1 97.6 maximum number of tokens in a batch to 4096, and accumulate four batches of gradients for one parameter update. we used the adam optimizer (kingma and ba, 2015) with learning rate 3 · 10−4 . we used a dropout rate of 0.1 and label smoothing (szegedy et al., 2016) with a coefficient 0.1. fine-tuning for all the downstream tasks in this section, when loading the pretrained models for fine-tuning, we do not load in the vocabulary embeddings nor the output layer weights. for the downstream task isarstep and metamathstep, we used four nvidia tesla t4 gpu with 16 cpu cores for training. we set the maximum number of tokens in a batch to 4096, and accumulated four batches of gradients for one parameter update. we trained the model for 200k updates. we used the adam optimizer, and we searched over the learning rates {3 · 10−4 , 7 · 10−4}, and warmup steps {4000, 8000}. we used a dropout rate of 0.1 and label smoothing with a coefficient 0.1. for the holist skip-tree task, we used tpus for running the experiments. we used a batch size of 256 sequences and trained the model for 1 million updates. architecture all experiments used the transformer base model from vaswani et al. (2017), i.e. 512 hidden size, 2048 filter size, 8 attention heads. for the isarstep and metamathstep task, we used 6 layers for both the encoder and decoder, implemented using fairseq (ott et al., 2019). for the holist skip-tree experiment, we used a somewhat modified transformer architecture with 8 encoder and 4 decoder layers of the same size as above in which the self-attention and attention over the encoder output were merged. evaluation during training, we kept track of the best validation tokenized bleu score 1 , and we used the model with validation bleu for evaluation on the test set. we report top-1 and top-10 accuracies. we consider an output sequence as correct if it matches the target sequence exactly. we performed a beam search with width 10. the top-1 accuracy is then defined as the percentage of the best output sequences that are correct. the top-n accuracy is defined as the percentage of target sequences appearing in the top n generated sequences. 4.2 isarstep the isarstep task is taken from li et al. (2020). isarstep is a task of predicting the missing intermediate propositions given surrounding propositions to bridge the gap between the goal and the current state of the proof. the dataset was mined from the public repository of formal proofs of the isabelle proof assistant (paulson, 1994). unlike holist and metamath, isarstep contains mostly declarative proofs, a proof style close to humans’ prose proofs. the dataset has a broad coverage of undergraduate and 1https://github.com/pytorch/fairseq/blob/master/fairseq/tasks/ translation.py#l396 6 table 3: test top-1, top-10 (%) accuracy on the metamathstep task. model top-1 acc. top-10 acc. no pretrain 67.7 76.5 lime deduct 68.8 77.4 lime abduct 68.8 76.1 lime induct 69.9 78.0 lime mix 69.1 77.9 research-level mathematics and computer science theorems. there are 820k, 5000, 5000 sequence pairs for the training, validation, and test sets with a maximum of 800 tokens in source sequences and 200 tokens in the target sequences. following li et al. (2020), during training, we use 512 as the maximum length for both the source and target, and truncated those that exceed the length to 512. for reporting, we evaluate all 5000 test examples regardless of their lengths. 50k 100k 150k 200k training steps 45 50 55 60 65 validation bleu: isarstep lime deduct lime induct lime abduct lime mix no pretrain figure 1: validation bleu along training on the isarstep task. the results on the isarstep task for four pretrained models and the baseline transformer model without pretraining is shown in table 1. we also include another baseline, hat transformer introduced in li et al. (2020), which is a specially designed hierarchical transformer architecture tailored to this task. we see the pretrained model achieved substantial improvement over the model trained from scratch as well as hat. notably, the model that was pretrained on abduct improved the top-10 accuracy from 33.1% to 41.0%, for almost 8% absolute improvement. the model pretrained on mix performed the best on top-1 accuracy, improving the baseline by 6.5% accuracy. we also showed the validation bleu scores along training in figure 1. we can see that the pretrained models learned much faster than the model trained from scratch. with around 50k steps of updates, the pretrained model already obtained better bleu scores than the best score achieved by the un-pretrained model. moreover, since the downstream task requires 200k steps of training with 4 gpus, the amount of computation spent on pretraining is only 2.5% of the downstream task, strongly demonstrating the efficiency of the proposed pretraining method. 4.3 holist skip-tree as the second mathematical reasoning benchmark we consider the holist skip-tree evaluation tasks by rabe et al. (2020). these tasks include two variants of type inference, predicting under which assumptions theorems hold, and completing equalities. all source expressions for these tasks are taken from the validation set of the theorem database of the holist proof logs (bansal et al.). the evaluations are done on a random sample of 1000 instances from the full evaluation sets. we initialized the model parameters with the pretrained weights and then repeated the experiments by rabe et al. (2020). that is, we trained the models for up to 1m parameter updates on the training set with batch size 256 and repeat the evaluation every 100k steps. in table 2 we present the best result from these 10 evaluation runs. we see a significant improvement in these reasoning tasks when the models are initialized with the pretrained weights. notably, on equation completion and missing assumptions task, we improved the beam search (with width 8) exact match rate performance from 46.3% to 51.7% and 41.8% to 47.9%. note that this is despite the amount of pretraining compute cost being negligible: it takes less than 1 percent of the cost of the downstream task training. pretraining used 1/20 number of the update steps (50k vs 1m) with 8 (and 4) times smaller batches (pretraining has much shorter sequence lengths, 128 vs. 1024 and 512, respectively). 7 table 4: comparisons to other pretraining tasks on isarstep task. model top-1 acc. top-10 acc no pretrain (li et al., 2020) 20.4 33.1 lime mix 26.9 40.4 pretrain on metamathstep 23.1 35.7 pretrain on wmt en-de 17.2 30.3 4.4 metamathstep compared to other itps, metamath is a low-level proving system: each proof step makes only a small step towards the goal. as such, each proof contains many more proof steps than in other itps: with 37, 000 theorems in the human-written theorem library, there are around 3 million proof steps. we extract the proof steps and use them to construct a sequence-to-sequence task following polu and sutskever (2020) (their proof step training objective). in this task, the model is asked to generate proofsteps given a goal, namely, the goal string is the source input, and proofsteps is the target output. we follow polu and sutskever (2020) and use their string representation for the goal and the proofsteps. instead of using subword tokenization in polu and sutskever (2020), we use a character-level representation for our task. following polu and sutskever (2020), we split theorems into train/valid/test theorems of size 35k, 1k, 1k, and associate all proof steps of a theorem with that split. for each dataset, we filter examples with lengths longer than 1024. this reduced the total number of proof steps to 1.4 million. for validation and test set, we randomly sample 3000 examples out of 40k (after filtering) and perform validation and test evaluations on them. in table 3 we present the impact of pretraining on our synthetic reasoning tasks on metamathstep. we also observe gains from pretraining on this dataset, with the model trained on induct task achieving 2.2% top-1 and 1.5% top-10 test accuracy improvement. similarly, as for the isarstep task, the computation spent on pretraining is only 2.5% of the downstream task. 5 ablation studies in this section, we perform ablation studies. additional ablation studies can be found in appendix c. 5.1 pretraining on formal reasoning and natural language tasks here we investigate how lime compares to pretraining on natural language or existing formal reasoning datasets. in this set of experiments, we pretrained three models on mix, metamathstep, and on the wmt 2016 english-to-germany (wmt en-de) translation task, and then we fine-tuned and evaluated these models on the isarstep task. we pretrained the model on metamathstep and wmt en-de for 200k steps with 4 gpus, which is 40 times more computation spent than on lime. due to the mismatch between vocabularies of the pretraining task and the downstream task, we do not load the vocabulary embeddings nor output layer weights. the results in table 4 show that pretraining on metamathstep did provide gains, though significantly smaller than gains provided by lime mix, despite their 40 times higher computational cost. moreover, pre-training on wmt translation had even a negative effect on the performance. we also conducted an analogous experiment with an evaluation on the metamathstep, which we present in appendix c. 5.2 do we need vocabulary embeddings for fine-tuning? as mentioned earlier, we did not load in the vocabulary embeddings from the pretrained models when we switched to fine-tuning on downstream tasks. even without loading the vocab embeddings, the pretrained models still improved the performance. in this ablation study, we investigate how much this decision has affected the results and whether vocabulary embeddings can help improve the performance even further. we performed the comparisons on isarstep. the task contains a token vocabulary of size 28336. we generated new synthetic tasks for the same vocabulary size, such that we can load the vocabulary embeddings and output layers when initializing the model for isarstep. 8 table 5: whether one needs to load vocabulary embeddings and output layer weights on isarstep tasks. model top-1 acc. top-10 acc no pretrain (li et al., 2020) 20.4 33.1 lime mix 26.9 40.4 lime mix + loading all weights 26.7 40.6 table 5 shows that this led to similar performance. this aligns with our expectation that the model should not learn content specific knowledge that is potentially stored in the vocabulary. these weights turn out to be non-essential for the final performance, supporting the evidence that the transformer learns inductive biases from the pretraining task. 6 does lime encode induction, deduction and abduction? although lime has shown to achieve substantial improvements across various benchmarks, it is not entirely clear that the specific synthetic tasks necessarily enforce the reasoning ability of induction, deduction and abduction. we would like to note that deduction, induction, and abduction are highlevel and philosophical concepts, and serve only as an inspiration for us to design the synthetic tasks. we do not expect the model will necessarily learn exactly these three capabilities. after all, we have chosen a particular implementation of "case", "rule" and "result". furthermore, we also design tasks mimic proof steps in formal theorem proving (see the rewrite task in appendix b.1), which also achieved excellent results. nevertheless, we believe lime is a first step towards building reasoning inductive biases, and provides many inspirations and directions for future work. 7 conclusion in this work, we encoded inductive biases for mathematical reasoning in the form of datasets. we created three synthetic tasks inspired by three reasoning primitives of deduction, induction, and abduction. we demonstrated that pretraining on these tasks (lime) significantly improved the performances across three mathematical reasoning benchmarks. notably, lime requires negligible computation compared to the downstream task, unlike being the dominating factor in previous pretraining methods. our work naturally poses many future research questions. could the primitive tasks provide similar gains for nlp tasks? are there similar primitive tasks for natural language reasoning? we also look forward to disentangling the effects of pretraining between learning content knowledge and inductive bias for all downstream tasks to better understand pre-training. 9 references kshitij bansal, sarah m. loos, markus n. rabe, christian szegedy, and stewart wilcox. holist: an environment for machine learning of higher order logic theorem proving. in 36th international conference on machine learning, icml 2019, long beach, california, usa, june 9-15, 2019. url http://proceedings.mlr.press/v97/bansal19a.html. kshitij bansal, christian szegedy, markus n. rabe, sarah m. loos, and viktor toman. learning to reason in large theories without imitation. arxiv preprint arxiv:1905.10501, 2019. francesco bellucci and ahti-veikko pietarinen. charles sanders peirce: logic. in the internet encyclopedia of philosophy, 2015. url https://iep.utm.edu/peir-log/. tom b. brown, benjamin mann, nick ryder, melanie subbiah, jared kaplan, prafulla dhariwal, arvind neelakantan, pranav shyam, girish sastry, amanda askell, sandhini agarwal, ariel herbert-voss, gretchen krueger, tom henighan, rewon child, aditya ramesh, daniel m. ziegler, jeffrey wu, clemens winter, christopher hesse, mark chen, eric sigler, mateusz litwin, scott gray, benjamin chess, jack clark, christopher berner, sam mccandlish, alec radford, ilya sutskever, and dario amodei. language models are few-shot learners. corr, abs/2005.14165, 2020. url https://arxiv.org/abs/2005.14165. alexis conneau and guillaume lample. cross-lingual language model pretraining. in advances in neural information processing systems, neurips 2019, vancouver, bc, canada, december 8-14, 2019, pages 7057–7067, 2019. url http://papers.nips.cc/paper/ 8928-cross-lingual-language-model-pretraining. maxwell crouse, ibrahim abdelaziz, cristina cornelio, veronika thost, lingfei wu, kenneth forbus, and achille fokoue. improving graph neural network representations of logical formulae with subgraph pooling. arxiv preprint arxiv:1911.06904, 2019. jacob devlin, ming-wei chang, kenton lee, and kristina toutanova. bert: pre-training of deep bidirectional transformers for language understanding. in jill burstein, christy doran, and thamar solorio, editors, proceedings of the 2019 conference of the north american chapter of the association for computational linguistics: human language technologies, naacl-hlt 2019, minneapolis, mn, usa, june 2-7, 2019, volume 1 (long and short papers), pages 4171– 4186. association for computational linguistics, 2019. doi: 10.18653/v1/n19-1423. url https://doi.org/10.18653/v1/n19-1423. li dong, nan yang, wenhui wang, furu wei, xiaodong liu, yu wang, jianfeng gao, ming zhou, and hsiao-wuen hon. unified language model pre-training for natural language understanding and generation. in advances in neural information processing systems, neurips 2019, vancouver, bc, canada, december 8-14, 2019, pages 13063–13075, 2019. richard evans, david saxton, david amos, pushmeet kohli, and edward grefenstette. can neural networks understand logical entailment? in international conference on learning representations, 2018. url https://openreview.net/forum?id=skzxck-0z. thibault gauthier, cezary kaliszyk, josef urban, ramana kumar, and michael norrish. tactictoe: learning to prove with tactics. journal of automated reasoning, pages 1–30, 2020. christopher hahn, frederik schmitt, jens u. kreber, markus n. rabe, and bernd finkbeiner. transformers generalize to the semantics of logics. arxiv preprint arxiv:2003.04218, 2020. jesse michael han. enhancing sat solvers with glue variable predictions. arxiv preprint arxiv:2007.02559, 2020. vincent j. hellendoorn, charles sutton, rishabh singh, petros maniatis, and david bieber. global relational models of source code. in 8th international conference on learning representations, iclr 2020, addis ababa, ethiopia, april 26-30, 2020. openreview.net, 2020. url https: //openreview.net/forum?id=b1lnbrntwr. sepp hochreiter and jürgen schmidhuber. long short-term memory. neural computation, 9(8): 1735–1780, 1997. 10 daniel huang, prafulla dhariwal, dawn song, and ilya sutskever. gamepad: a learning environment for theorem proving. in 7th international conference on learning representations, iclr 2019, new orleans, la, usa, may 6-9, 2019. openreview.net, 2019. url https://openreview. net/forum?id=r1xwkor9y7. mandar joshi, danqi chen, yinhan liu, daniel s. weld, luke zettlemoyer, and omer levy. spanbert: improving pre-training by representing and predicting spans. transactions of the association for computational linguistics, 8:64–77, 2020. doi: 10.1162/tacl\_a\_00300. url https: //doi.org/10.1162/tacl_a_00300. diederik p. kingma and jimmy ba. adam: a method for stochastic optimization. in yoshua bengio and yann lecun, editors, 3rd international conference on learning representations, iclr 2015, san diego, ca, usa, may 7-9, 2015, conference track proceedings, 2015. url http://arxiv.org/abs/1412.6980. guillaume lample and françois charton. deep learning for symbolic mathematics. in 8th international conference on learning representations, iclr 2020, addis ababa, ethiopia, april 26-30, 2020. openreview.net, 2020. url https://openreview.net/forum?id= ske31kbtpr. yann lecun, patrick haffner, léon bottou, and yoshua bengio. object recognition with gradientbased learning. in shape, contour and grouping in computer vision, page 319, berlin, heidelberg, 1999. springer-verlag. isbn 3540667229. gil lederman, markus rabe, sanjit seshia, and edward a lee. learning heuristics for quantified boolean formulas through reinforcement learning. in international conference on learning representations, 2020. wenda li, lei yu, yuhuai wu, and lawrence c. paulson. modelling high-level mathematical reasoning in mechanised declarative proofs. arxiv preprint arxiv:2006.09265, 2020. yinhan liu, myle ott, naman goyal, jingfei du, mandar joshi, danqi chen, omer levy, mike lewis, luke zettlemoyer, and veselin stoyanov. roberta: a robustly optimized bert pretraining approach. corr, abs/1907.11692, 2019. url http://arxiv.org/abs/1907.11692. thang luong, hieu pham, and christopher d. manning. effective approaches to attention-based neural machine translation. in lluís màrquez, chris callison-burch, jian su, daniele pighin, and yuval marton, editors, proceedings of the 2015 conference on empirical methods in natural language processing, emnlp 2015, lisbon, portugal, september 17-21, 2015, pages 1412– 1421. the association for computational linguistics, 2015. doi: 10.18653/v1/d15-1166. url https://doi.org/10.18653/v1/d15-1166. r. thomas mccoy, e. grant, p. smolensky, t. griffiths, and tal linzen. universal linguistic inductive biases via meta-learning. proceedings of cogsci, abs/2006.16324, 2020. myle ott, sergey edunov, alexei baevski, angela fan, sam gross, nathan ng, david grangier, and michael auli. fairseq: a fast, extensible toolkit for sequence modeling. in waleed ammar, annie louis, and nasrin mostafazadeh, editors, proceedings of the 2019 conference of the north american chapter of the association for computational linguistics: human language technologies, naacl-hlt 2019, minneapolis, mn, usa, june 2-7, 2019, demonstrations, pages 48–53. association for computational linguistics, 2019. doi: 10.18653/v1/n19-4009. url https://doi.org/10.18653/v1/n19-4009. aditya paliwal, sarah m. loos, markus n. rabe, kshitij bansal, and christian szegedy. graph representations for higher-order logic and theorem proving. in the thirty-fourth aaai conference on artificial intelligence, aaai 2020, the thirty-second innovative applications of artificial intelligence conference, iaai 2020, the tenth aaai symposium on educational advances in artificial intelligence, eaai 2020, new york, ny, usa, february 7-12, 2020, pages 2967–2974. aaai press, 2020. url https://aaai.org/ojs/index.php/aaai/article/view/5689. isabel papadimitriou and dan jurafsky. learning music helps you read: using transfer to study linguistic structure in language models. in proceedings of the 2020 conference on empirical methods 11 in natural language processing (emnlp), pages 6829–6839, online, november 2020. association for computational linguistics. url https://www.aclweb.org/anthology/2020. emnlp-main.554. charles sanders peirce. reasoning and the logic of things: the cambridge conferences lectures of 1898. harvard university press, 1992. bartosz piotrowski and josef urban. guiding inferences in connection tableau by recurrent neural networks. in christoph benzmüller and bruce miller, editors, intelligent computer mathematics, pages 309–314, cham, 2020. springer international publishing. isbn 978-3-030-53518-6. stanislas polu and ilya sutskever. generative language modeling for automated theorem proving. corr, abs/2009.03393, 2020. url https://arxiv.org/abs/2009.03393. markus n. rabe, dennis lee, kshitij bansal, and christian szegedy. mathematical reasoning via self-supervised skip-tree training. arxiv preprint arxiv:2006.04757, 2020. alec radford, jeffrey wu, rewon child, david luan, dario amodei, and ilya sutskever. language models are unsupervised multitask learners. in openai blog, 2018. url https://d4mucfpksywv.cloudfront.net/better-language-models/ language_models_are_unsupervised_multitask_learners.pdf. colin raffel, noam shazeer, adam roberts, katherine lee, sharan narang, michael matena, yanqi zhou, wei li, and peter j. liu. exploring the limits of transfer learning with a unified text-totext transformer. j. mach. learn. res., 21:140:1–140:67, 2020. url http://jmlr.org/ papers/v21/20-074.html. david saxton, edward grefenstette, felix hill, and pushmeet kohli. analysing mathematical reasoning abilities of neural models. in proceedings of international conference on learning representations (iclr), 2019. franco scarselli, marco gori, ah chung tsoi, markus hagenbuchner, and gabriele monfardini. the graph neural network model. ieee transactions on neural networks, 20(1):61–80, 2008. imanol schlag, paul smolensky, roland fernandez, nebojsa jojic, jürgen schmidhuber, and jianfeng gao. enhancing the transformer with explicit relational encoding for math problem solving. corr, abs/1910.06611, 2019. url http://arxiv.org/abs/1910.06611. daniel selsam and nikolaj bjørner. guiding high-performance sat solvers with unsat-core predictions. in international conference on theory and applications of satisfiability testing, pages 336–353. springer, 2019. daniel selsam, matthew lamm, benedikt bünz, percy liang, leonardo de moura, and david l. dill. learning a sat solver from single-bit supervision. in 7th international conference on learning representations, iclr 2019, new orleans, la, usa, may 6-9, 2019. openreview.net, 2019. url https://openreview.net/forum?id=hjmc_ia5tm. kaitao song, xu tan, tao qin, jianfeng lu, and tie-yan liu. mass: masked sequence to sequence pre-training for language generation. in 36th international conference on machine learning, icml 2019, long beach, california, usa, june 9-15, 2019, 2019. url http://proceedings. mlr.press/v97/song19d.html. christian szegedy, vincent vanhoucke, sergey ioffe, jon shlens, and zbigniew wojna. rethinking the inception architecture for computer vision. in proceedings of the ieee conference on computer vision and pattern recognition, pages 2818–2826, 2016. josef urban and jan jakub˚uv. first neural conjecturing datasets and experiments. in christoph benzmüller and bruce miller, editors, intelligent computer mathematics, pages 315–323, cham, 2020. springer international publishing. isbn 978-3-030-53518-6. pashootan vaezipoor, gil lederman, yuhuai wu, chris j. maddison, roger b. grosse, edward a. lee, sanjit a. seshia, and fahiem bacchus. learning branching heuristics for propositional model counting. corr, abs/2007.03204, 2020. url https://arxiv.org/abs/2007.03204. 12 ashish vaswani, noam shazeer, niki parmar, jakob uszkoreit, llion jones, aidan n. gomez, lukasz kaiser, and illia polosukhin. attention is all you need. in proceedings of advances in neural information processing systems (neurips), 2017. mingzhe wang, yihe tang, jian wang, and jia deng. premise selection for theorem proving by deep graph embedding. in advances in neural information processing systems, pages 2786–2796, 2017. qingxiang wang, chad brown, cezary kaliszyk, and josef urban. exploration of neural machine translation in autoformalization of mathematics in mizar. proceedings of acm sigplan international conference on certified programs and proofs, 2020. alex warstadt and samuel r. bowman. can neural networks acquire a structural bias from raw linguistic data? proceedings of cogsci, 2020. yuhuai wu, albert jiang, jimmy ba, and roger grosse. int: an inequality benchmark for evaluating generalization in theorem proving. arxiv preprint arxiv:2007.02924, 2020. keyulu xu, jingling li, mozhi zhang, simon s du, ken-ichi kawarabayashi, and stefanie jegelka. what can neural networks reason about? in iclr 2020, 2020. kaiyu yang and jia deng. learning to prove theorems via interacting with proof assistants. in proceedings of international conference on machine learning (icml), 2019. zhilin yang, zihang dai, yiming yang, jaime carbonell, russ r salakhutdinov, and quoc v le. xlnet: generalized autoregressive pretraining for language understanding. in advances in neural information processing systems, neurips 2019, vancouver, bc, canada, december 8-14, 2019, 2019. jingqing zhang, yao zhao, mohammad saleh, and peter j. liu. pegasus: pre-training with extracted gap-sentences for abstractive summarization. in 37th international conference on machine learning, icml 2020, vienna, austria, 2020, volume 119. pmlr, 2020. 13 appendix a synthetic task generation pseudocode algorithm 1 1: function generate_tuple( vocabulary size s) 2: vocabulary v ← {1, 2, . . . , s}. . use an integer representation of symbols. 3: math symbol set m ← sample(v, n=44, replacement=false). . sample 44 distinct symbols. 4: rule symbol set r ← sample(v\m, n=20, replacement=false). . sample 20 distinct symbols. 5: rule r ← sample(m s r, n=random(5,20), replacement=false). . sample a sequence of symbols of length between 5 and 20. 6: case dictionary c ← {}. 7: for s in r do 8: case dictionary c[s] ← sample(m, n=random(2,8), replacement=true). . sample a sequence of symbols for each rule symbol, of length of length between 2 and 8. 9: end for 10: result r 0 ← rule r. . set result string r 0 to be the same as rule string r. 11: for s in r do 12: substitute(r 0 , s, c[s]). . substitute every rule symbol s in result string r 0 with previously randomly sampled string c[s]. 13: end for 14: return math symbol set m, rule symbol set r, rule r, case c, result r 0 . 15: end function appendix b other synthetic tasks in this section, we give descriptions of other variants of the synthetic tasks we considered than the ones introduced in the main paper. appendix b.1 re w r i t e and re w r i t e_m u l t i s t e p we propose a rewrite task, inspired by the rewrite tactic used in interactive theorem provers. the rewrite task requires the model to rewrite a string according to a rule transformation. one example of the task is: source: a + b − c a + b = b + a target: b + a − c “a + b = b + a“ is the rule transformation, which is applied to the lhs string “a + b − c”. the model needs to predict the rhs string as the result of the rule application, i.e., b + a − c. besides rule symbols and math symbols, we also require the third set of symbols, named as "string symbols". for the ease of our discussion, we we will think of math symbols as the union of those operators used in mathematics (e.g., “+ − ∗ = ()&”), rule symbols as upper case letters (e.g., a, b, c . . . ), and string symbols as lower case letters (e.g., a, b, c . . . ). we first sample a random string as the lhs string, consisting of math symbols and string symbols (e.g., a + b − c). we sample a sub-string of the lhs string, and replace the string symbols in the sub-string with rule symbols. for example, we sample and obtain the substring a + b from a + b − c, and we replace a, b with rule symbols a, b. this then forms the lhs of the rule transformation, a + b, with the substitution dictionary {a : a, b : b}. we then sample the rhs of the rule transformation from the union of rule symbols a and b, and all math symbols, e.g., b + a. this gives the rule transformation a + b = b + a. we substitute the value of the substitution dictionary for each rule symbol in the rhs rule, and then substitute back to the original lhs string to obtain b + a − c. the task example is constructed by using the lhs string and the rule transformation as the source input, and use the result of the rule transformation as the target. we further introduce a multi-step version of the rewrite task: rewrite_multistep. in this task, the source may contain more than one rewrite rule, and the target is the result of applying all the rewrite rules in a sequence. this task is motivated from the need to perform multi-step planning in 14 table 6: test top-1, top-10 (%) accuracy on the isarstep task. model top-1 acc. top-10 acc. no pretrain (li et al., 2020) 20.4 33.1 hat (li et al., 2020) 22.8 35.2 lime deduct 24.7 37.7 lime abduct 26.7 41.0 lime induct 23.9 38.8 lime mix 26.9 40.4 lime rewrite 26.0 38.6 lime rewrite_multistep 28.6 43.9 lime induct_v2 25.6 39.8 lime induct_v3 25.0 38.8 lime induct_rewrite 25.8 39.5 mathematical reasoning tasks. during pre-training, for each training example, we uniformly sample the number of rewrite steps from 1 to 5. appendix b.2 other variants of in d u c t task we introduce three other variants of the induct task. 1. induct_v2: we move the case dictionary from the source input to the target output. this makes the task significantly harder, which requires the agent to synthesize a rule and a possible explanation (case) to explain the result. 2. induct_v3: instead of providing the case dictionary, we provide two result strings, coming from the same rule. namely, we sample two case dictionaries, and applying each to the rule string to obtain two result strings. both result strings are used as source, and the target is the rule string. 3. induct_rewrite: we also create a “induction” version of the rewrite task. in this task, the source is the lhs string concatenated with the rhs string, that is the result of the rewrite. the target is the rewrite rule that is used to do the rewrite. appendix b.3 a full comparison of all synthetic tasks in this section we present a full comparison for all synthetic tasks. we followed the training protocol in 4.1 and evaluate the method on isarstep. the results are reported in table 6. we can see that the rewrite_multistep achieved the best performance across all synthetic tasks, surpassing the baseline by 8.2% for top-1 accuracy and 10.8% for top-10 accuracy. this indicates the inductive bias for long horizon reasoning encoded in rewrite_multistep is very useful for the reasoning task. appendix c more ablation studies appendix c.1 does the vocabulary size matter? in this section, we investigate whether the vocabulary size s in the synthetic task generation algorithm has an effect on the performance. we used the rewrite task for the experiment in this section. we generated datasets of various vocabulary sizes, 100, 512, 1000, 5000, 25000. we used the same curriculum learning for pre-training as described in 4.1 on larger vocabulary sizes: first training on the rewrite task of vocabulary size 100 for 10k steps, then training on each individual dataset for another 10k steps. we compare the performance on the downstream task isarstep. the results are presented in table 7. we see that when the vocabulary size is equal or larger than 512, the performance were similar. the smallest vocabulary size 100 obtained the worst performance among all, and all the other four models achieved similar bleu scores. the model trained on the largest vocabulary achieved best performance on top-1 accuracy and top-10 accuracy. the results show there is a non-trivial effect of the vocabulary size of the synthetic task to the performance of the 15 downstream task. hence we use vocabulary size of 1000 for all the experiments in the main paper. we leave investigations of the causes to future work. table 7: vocabulary sizes’ effects on the isarstep task. model top-1 acc. top-10 acc no pretrain 20.4 33.1 lime on rewrite, s = 100 24.1 37.5 lime on rewrite, s = 512 25.4 38.8 lime on rewrite, s = 1000 26.0 38.6 lime on rewrite, s = 5000 25.8 38.5 lime on rewrite, s = 25000 27.4 40.9 appendix c.2 pre-training on isarstep for metamathstep following section 5.1, we performed pre-training on isarstep for metamathstep. the result is shown in table 8. in contrast to metamath helping isarstep, we see that pretraining on isarstep task did not help the downstream task metamathstep. we hypothesize that this could be due to metamathstep task is closer to the lime tasks than isarstep, and hence providing more gains than the opposite direction. we leave investigations to the future versions. table 8: pretraining on isarstep for the metamathstep task. model top-1 acc. top-10 acc. no pretrain 67.7 76.5 lime mix 69.1 77.9 pretrain on isarstep 67.0 76.1 appendix c.3 does lime help lstms? in this section, we investigate if lime also helps other architectures than transformers. in particular, we applied lime to two lstm based architectures: 1. vanilla lstm, 2. lstm with attention mechanism. the vanilla lstm is a stacking lstm with 4 layers, each with 1000 cells, and 1000- dimensional embeddings. the lstm with attention architecture is taken from luong et al. (2015), also with 4 layers, 1000 cells and 1000-dimensional embeddings. we evaluate on the isarstep task, and compared a model trained from scratch and a model pre-trained on lime abduct task. we used the same training protocol as described in 4.1. the results are shown in table 9, along with the results on transformer. we observe that lime improved lstm as well as lstm with attention, but the improvements were small compared to transformer. specifically, if we compare top-1 accuracy, we can see that lime improved lstm from 5.5% to 6.9%, lstm with attention from 12.3% to 13.4%, and transformer from 20.4% to 26.7%. this observation is aligned with our hypothesis that the transformer is a malleable architecture and hence it is capable of learning architectural inductive biases from datasets. this is mainly attributed to the potential of learning dynamic attention graphs in self-attention layers. we note that this still warrants further investigation as the performance of these architectures are not at the same level, and that may also lead to different improvements. table 9: comparing lime’s benefits on lstms on the isarstep task model top-1 acc. top-10 acc. lstm 5.5 11.3 lstm + lime abduct 6.9 14.3 lstm + attention 12.3 22.7 lstm + attention + lime abduct 13.4 26.3 transformer 20.4 33.1 transformer + lime abduct 26.7 41.0 16. deep learning based prediction of alzheimer’s disease from magnetic resonance images manu subramoniam, aparna t. r., anurenjan p. r., and sreeni k. g. computer vision lab, dept. of ece, college of engineering, thiruvananthapuram tve18ecsp-p04@cet.ac.in, aparnatr96@cet.ac.in, anurenjan@cet.ac.in, sreenikg@cet.ac.in , abstract. alzheimer’s disease (ad) is an irreversible, progressive neuro degenerative disorder that slowly destroys memory and thinking skills and eventually, the ability to carry out the simplest tasks. in this paper, a deep neural network based prediction of ad from magnetic resonance images (mri) is proposed. the state of the art image classification networks like vgg, residual networks (resnet) etc. with transfer learning shows promising results. performance of pre-trained versions of these networks are improved by transfer learning. resnet based architecture with large number of layers is found to give the best result in terms of predicting different stages of the disease. the experiments are conducted on kaggle dataset. keywords: cnn, resnet, mri, alzheimer’s disease, transfer learning 1 introduction alzheimer’s disease (ad) is a progressive neuro degenerative disease that affected nearly 50 million people worldwide [1]. the disease causes an irreversible damage to the brain that affects cognition, memory and other function and leads to the death of the individual from complete brain failure. the economic consideration of the disease is huge and well studied [2]. since the disease in incurable, early diagnosis and medications for delaying the progression are the only treatment available [3]. genetic bio-markers like amyloidβ precursor protein (a βpp) could be found from blood tests [4], which may be used for diagnosing ad. ad results in senile plaques and neurofibrillary tangles throughout the brain which is also considered as a definitive bio-marker. these plaques and tangles tend to shrink the brain volume. this shrinkage is evident in mr images and are used as a criteria for clinical diagnosis [5]. the rest of the paper is organized as follows. the related prior work is discussed in section 2. the details of the proposed work can be found in 3. this section also contains details about the dataset used and the details about the various architectures experimented with. the experiments and results are discussed in section 4. this is followed by a summary of the work in section 5. arxiv:2101.04961v1 [eess.iv] 13 jan 2021 2 manu et al. 2 prior work alzheimer’s disease (ad) is an irreversible, progressive brain disorder with no existing treatment for curing the disease. hence a great deal of effort has been made to develop strategies for early detection, especially at pre-symptomatic stages of the disease. in particular, advanced neuroimaging techniques, such as magnetic resonance imaging (mri) have been used to identify ad-related diseases. to predict ad in subjects with mild cognitive impairment (mci), simon f. eskildsen et al. [7] investigated the possibility of using patterns of cortical thickness measurements. specific patterns of atrophy were identified and features were selected as regions of interest from these patterns. in claudia plant et al. [8] a data mining framework in combination with three different classifiers including support vector machine (svm), bayes statistics, and voting feature intervals (vfi) were used to derive a quantitative index of pattern matching for the prediction. in this study, the multivariate methods of pattern matching reach a clinically relevant accuracy for the a priori prediction of the progression from mci to ad. to jointly predict multiple variables from multi-modal data daoqiang zhang et al. [9] studies multimodal multi-task (m3t) learning method. multi-task feature selection to selects the common subset of relevant features for multiple variables from each modality fuses with multi-modal support vector to predict multiple (regression and classification) variables. mri surface morphometry mapping is used to evaluate local deformations of the hippocampus, parahippocampal gyrus, and entorhinal cortex to predict conversion from mci to ad in d.p. devanand et al. [10]. amongst the traditional machine learning methods, svm is the most popular, which extract high-dimensional, informative features to predict classification models that facilitate the automation of clinical diagnosis in rathore et al. [11]. however, feature extraction and definition relies on manual outlining of brain structures, which is laborious and complex image pre-processing, which is computationally demanding and time-consuming. to overcome these difficulties, deep learning, an emerging area of machine learning research that uses raw neuroimaging data to generate features is attracting considerable attention in the field of large scale, high-dimensional medical imaging analysis in plis et al. [12]. so it requires little or no image pre-processing and can automatically infer an optimal representation of the data from the raw images without requiring prior feature selection, resulting in a more objective and less bias-prone process. recently, deep learning has been successfully applied to the alzheimer’s disease neuroimaging initiative (adni) [13] dataset to identify ad patients in vieira et al. [14]. deep learning algorithms, without a priori feature selection (considering gray matter [gm] volumes as input) is used in the prediction of ad development using adni structural mri scans in suk et al. [15]. convolutional neural networks (cnns) using 3d t1-weighted images from the adni dataset is used by silvia basaia et al., weiming lin et al. [16] [17]. to avoid using complicated activations, response normalization, or max-pooling, silvia basaia et al. [16]used standard convolutional layers with the stride of 2 (‘all convolutional network’ )instead of max-pooling layers. thus there is a reduction in the number of network parameters. in weiming lin et al. [17] mri alzheimer’s disease prediction 3 images are prepared with age-correction and, local patches are extracted from these images. a special extreme learning machine to avoids the random generation of the input weight matrix is chosen for classification with both cnn-based features and freesurfer [18]based features. karteek popuri et al. [19] developed a method to quantify the structural patterns from a structural mri to develop a score for similarity to patterns seen in dementia of alzheimer’s type dat images. so employed ensemble-learning framework to create an aggregate measure of neurodegeneration in the brain. v.p. subramanyam rallabandi et al. [20]used freesurfer analysis to measure regional cortical thickness of both left and right hemispheres. the non-linear support vector machine using a radial basis function kernel is used for classification of different stages of dementia. manhua liu et al. [21] proposed a multi-modal deep learning framework based on cnn for joint automatic hippocampal segmentation and ad classification. a multi-task deep cnn model is constructed for jointly learning hippocampal segmentation and classification. to learn these features of patches, a 3d densely connected convolutional networks (3d densenet) is constructed. therefore, deep learning algorithms are better suited for detecting subtle abnormalities. 3 proposed method in this paper, the axial slices of mri images is used as the input data for the classification task. as shown in fig. 1, the mri slices are fed to the neural network which performs feature extraction and classification. the classifier is basically a cnn model that labels the image into one of the four classes -nondemented, very mild demented, mild demented and moderately demented. a pre-trained resnet-101 model have been used for the classifier cnn block. the details of resnet-101 architecture is discussed in section 3.1. this gives most accurate classifications for the application. 3.1 architecture in the architecture, we make use of residual neural networks (resnet-101) for the classification purposes. resnet architecture is created by stacking up residual blocks, where each residual block consists of 3 layers - 1×1, 3×3 and 1×1. this is referred to as the bottleneck building block [6]. the 1×1 layers preceding and following the 3×3 layer are responsible for dimensionality reduction and restoration respectively. table 1 shows the architecture of resnet-101 along with details of the building blocks and the number of blocks stacked. down sampling of the input image is done by conv3 1, conv4 1 and conv5 1. the bottleneck blocks for each layers are separately given in fig. 2 we have classified with vanilla-dense neural network (dnn) architecture. the first layer is the input layer which flattens the vectors followed by 3 units of dense layers used in this network. 2 units of dense layer is with relu activation and 1 unit of dense layer is with softmax activation is used. also, we have performed classification with the sequential addition of convolutional layer followed by this 4 manu et al. fig. 1. block diagram illustration of the proposed method of alzheimer’s disease classification from brain mri image slices. layer name output size layer conv1 112×112 7×7, 64 stride 2 3×3 maxpool, stride 2 conv2 x 56×56 1 × 1, 64 3 × 3, 64 1 × 1, 256 × 3 conv3 x 28×28 1 × 1, 128 3 × 3, 128 1 × 1, 512 × 4 conv4 x 14×14 1 × 1, 256 3 × 3, 256 1 × 1, 1024 × 23 conv5 x 7×7 1 × 1, 512 3 × 3, 512 1 × 1, 2048 × 3 1×1 average pool, 1000-d fc, softmax table 1. the details of resnet-101 architecture used in the proposed method showing the building blocks (in brackets), along with number of blocks stacked. dnn architecture. with 2 units of the convolutional layer of 3x3 filter with a stride 1 and 1 unit of softmax layer of 2x2 filter with a stride 2, which is shown in table 2. 3.2 dataset used the data consists of 6400 magnetic resonance images collected and released as part of a kaggle competition [22]. mr images are categorized as non-demented, alzheimer’s disease prediction 5 fig. 2. bottleneck building blocks of different layers for resnet-101 architecture. a. conv2 x layer b. conv3 x layer c. conv4 x layer d. conv5 x layer. very mildly demented, mildly demented and moderately demented based on the level of neurological degeneration. the complete dataset is divided between a non overlapping train set and test set. training set has 5121 images and the test set consists of 1279 images. some classes like moderately demented is under represented. in order to balance the class variability, data augmentation is used during training. 4 experimental results for the experiment setup, google colab is used. gpu hardware accelerator is used during the training process. the kaggle ad dataset with 5121 data samples for training and 1279 test samples for testing. the samples were spread across 4 6 manu et al. classes - non-demented very mild demented, mild demented and moderately demented. fastai [23] is used for programming the network. in order to balance the data set, data augmentation is used. sl. no. architecture accuracy in % 1 vanilla dnn 95.31 2 cnn-dnn 95.32 table 2. performance comparison of different neural networks initialized with random weights class model accuracy precision recall f1–score mild demented vgg 16 98.63 0.93 0.98 0.95 vgg 19 94.04 0.72 0.87 0.79 resnet 18 98.54 0.98 0.93 0.95 resnet 34 94.63 0.75 0.89 0.81 resnet 50 96.29 0.81 0.94 0.87 resnet 101 99.51 0.99 0.98 0.98 moderate demented vgg 16 100 1.0 1.0 1.0 vgg 19 100 1.0 1.0 1.0 resnet 18 100 1.0 1.0 1.0 resnet 34 99.80 1.0 0.80 0.89 resnet 50 100 1.0 1.0 1.0 resnet 101 100 1.0 1.0 1.0 non demented vgg 16 96.78 0.97 0.96 0.97 vgg 19 87.60 0.90 0.85 0.87 resnet 18 97.85 0.96 0.99 0.98 resnet 34 89.16 0.89 0.89 0.89 resnet 50 92.29 0.93 0.91 0.92 resnet 101 99.61 0.99 1.0 1.0 very mild demented vgg 16 96.39 0.95 0.95 0.95 vgg 19 85.35 0.80 0.79 0.80 resnet 18 97.75 0.98 0.96 0.97 resnet 34 86.91 0.85 0.80 0.82 resnet 50 91.89 0.90 0.88 0.89 resnet 101 99.71 1.0 0.99 1.0 table 3. performance comparison of the proposed classification method (based on resnet-101) with other existing algorithms. comparison is done based on the parameters accuracy, precision, recall and f1-score. table 2 shows the accuracy values for different neural networks like simple vanilla dnn, cnn etc. vanilla dnn consists of 3-hidden layers. relu is used as the activation function. cnn-dnn has initial 3 cnn layers followed by 3 dnn alzheimer’s disease prediction 7 layers. from the accuracy values it can be seen that accuracy improves as we move from simple dnn to cnn-dnn based network. inspired by the fact that a simple cnn-dnn model worked well on the test data, we tried with the classic vgg-16 [24] network. here a pretrained vgg-16 network is taken and the last layer is replaced with 3 dnn layers. retraining the network with the kaggle data for a few epochs is done. results listed in table 3 shows that vgg-16 with transfer learning helps to improve the prediction. we also tried this transer learning approach with classic networks like vgg-19, resnet-18 [25], resnet-34, resnet-50 and resnet-101. it can be seen that among the vgg architectures, the vgg-16 is better performing than vgg-19. among the residual neural network architectures, the accuracy increases from resnet-18 to resnet-101. for all methods, the number of epochs is limited to 75. the detailed experimental results are shown in table 3. the parameters accuracy, precision, recall, and f1-score are used for performance evaluation. accuracy is the fraction of total samples that were classified by the classifier accuracy = t p + t n t p + t n + f p + f n where tp refers to the number of predictions where the classifier correctly predicts the positive class as positive tn refers to the number of predictions where the classifier correctly predicts the negative class as negative fp refers to the number of predictions where the classifier incorrectly predicts the negative class as positive fn refers to the number of predictions where the classifier incorrectly predicts the positive class as negative precision refers to the fraction of predictions as a positive class were actually positive p recision = t p t p + f p recall refers to what fraction of all positive samples were correctly predicted as positive by the classifier accuracy = t p t p + f n f1-score is given by f1 − score = 2t p 2t p + f p + f n 5 summary this paper proposes a deep neural network based classification of mri data. we use pretrained networks like vggnet and resnet, and retraining is done with 8 manu et al. the in domain data. experiments done with kaggle data shows promising results. resnet-101 architecture beats all others in terms of all evaluation metrics. future work should focus on experiments with clinically validated datasets like adni, oasis etc. also the possibility of exploiting multi modal cues like pet scans, blood test evaluations, mmse scores etc could be pursued. references 1. patterson, christina. “the state of the art of dementia research: new frontiers.” world alzheimer report (2018). 2. meek, patrick d., e. kristin mckeithan, and glen t. schumock. “economic considerations in alzheimer’s disease.” pharmacotherapy: the journal of human pharmacology and drug therapy 18.2p2 (1998): 68-73. 3. dubois, bruno, et al. “advancing research diagnostic criteria for alzheimer’s disease: the iwg-2 criteria.” the lancet neurology 13.6 (2014): 614-629. 4. huynh, rose ann, and chandra mohan. “alzheimer’s disease: biomarkers in the genome, blood, and cerebrospinal fluid.” frontiers in neurology 8 (2017): 102. 5. scheltens, philip, et al. “atrophy of medial temporal lobes on mri in” probable” alzheimer’s disease and normal ageing: diagnostic value and neuropsychological correlates.” journal of neurology, neurosurgery and psychiatry 55.10 (1992): 967-972. 6. k. he, x. zhang, s. ren and j. sun, “deep residual learning for image recognition,” 2016 ieee conference on computer vision and pattern recognition (cvpr), las vegas, nv, 2016, pp. 770-778, doi: 10.1109/cvpr.2016.90. 7. eskildsen, simon f., et al. ”prediction of alzheimer’s disease in subjects with mild cognitive impairment from the adni cohort using patterns of cortical thinning.” neuroimage 65 (2013): 511-521. 8. plant, claudia, et al. ”automated detection of brain atrophy patterns based on mri for the prediction of alzheimer’s disease.” neuroimage 50.1 (2010): 162-174. 9. zhang, daoqiang, dinggang shen, and alzheimer’s disease neuroimaging initiative. ”multi-modal multi-task learning for joint prediction of multiple regression and classification variables in alzheimer’s disease.” neuroimage 59.2 (2012): 895-907. 10. devanand, d. p., et al. ”mri hippocampal and entorhinal cortex mapping in predicting conversion to alzheimer’s disease.” neuroimage 60.3 (2012): 1622-1629. 11. rathore, saima, et al. ”a review on neuroimaging-based classification studies and associated feature extraction methods for alzheimer’s disease and its prodromal stages.” neuroimage 155 (2017): 530-548. 12. plis, sergey m., et al. ”deep learning for neuroimaging: a validation study.” frontiers in neuroscience 8 (2014): 229. 13. http://www.loni.ucla.edu/adni 14. vieira, sandra, walter hl pinaya, and andrea mechelli. ”using deep learning to investigate the neuroimaging correlates of psychiatric and neurological disorders: methods and applications.” neuroscience & biobehavioral reviews 74 (2017): 58-75. 15. suk, heung-il, et al. ”deep ensemble learning of sparse regression models for brain disease diagnosis.” medical image analysis 37 (2017): 101-113. 16. basaia, silvia, et al. ”automated classification of alzheimer’s disease and mild cognitive impairment using a single mri and deep neural networks.” neuroimage: clinical 21 (2019): 101645. 17. lin, weiming, et al. ”convolutional neural networks-based mri image analysis for the alzheimer’s disease prediction from mild cognitive impairment.” frontiers in neuroscience 12 (2018): 777. alzheimer’s disease prediction 9 18. surfer.nmr.mgh.harvard.edu 19. popuri, karteek, et al. ”using machine learning to quantify structural mri neurodegeneration patterns of alzheimer’s disease into dementia score: independent validation on 8,834 images from adni, aibl, oasis, and miriad databases.” human brain mapping 41.14 (2020): 4127-4147. 20. rallabandi, vp subramanyam, et al. ”automatic classification of cognitively normal, mild cognitive impairment and alzheimer’s disease using structural mri analysis.” informatics in medicine unlocked (2020): 100305. 21. liu, manhua, et al. ”a multi-model deep convolutional neural network for automatic hippocampus segmentation and classification in alzheimer’s disease.” neuroimage 208 (2020): 116459. 22. dubey, sarvesh.“alzheimer’s dataset (4 class of images).” https://www.kaggle.com/tourist55/alzheimers-dataset-4-class-of-images (2019) 23. howard, jeremy, and sylvain gugger.“fastai: a layered api for deep learning.” information 11.2 (2020): 108. 24. simonyan, karen, and andrew zisserman. ”very deep convolutional networks for large-scale image recognition.” arxiv preprint arxiv:1409.1556 (2014). 25. he, kaiming, et al. ”deep residual learning for image recognition.” proceedings of the ieee conference on computer vision and pattern recognition. 2016. frea-unet: frequency-aware u-net for modality transfer hajar emami, qiong liu, ming dong hajar.emami.gohari@wayne.edu, qiongliu1@gmail.com, mdong@wayne.edu abstract. while positron emission tomography (pet) imaging has been widely used in diagnosis of number of diseases, it has costly acquisition process which involves radiation exposure to patients. however, magnetic resonance imaging (mri) is a safer imaging modality that does not involve patient’s exposure to radiation. therefore, a need exists for an efficient and automated pet image generation from mri data. in this paper, we propose a new frequency-aware attention u-net for generating synthetic pet images. specifically, we incorporate attention mechanism into different u-net layers responsible for estimating low/high frequency scales of the image. our frequency-aware attention unet computes the attention scores for feature maps in low/high frequency layers and use it to help the model focus more on the most important regions, leading to more realistic output images. experimental results on 30 subjects from alzheimers disease neuroimaging initiative (adni) dataset demonstrate good performance of the proposed model in pet image synthesis that achieved superior performance, both qualitative and quantitative, over current state-of-the-arts. 1 introduction positron emission tomography (pet), a nuclear medical imaging technology offers potential for diagnosing a number of neurological diseases such as alzheimer, epilepsy, head and neck cancer by reflecting tissue metabolic activity in brain. however, obtaining high-quality pet images is costly and requires radioactive substance injection into the body, which may cause side effects to patients. due to these reasons, baseline datasets usually have smaller number of pet cases compare to other modalities. for example, in alzheimer’s disease neuroimaging initiative (adni) database, only approximately half of subjects have pet scans. on the other hand, magnetic resonance imaging (mri) has excellent soft tissue contrast with anatomical details that does not involve patient’s exposure to radiation. in order to circumvent the problems of pet imaging acquisition and streamline clinical efficiency, recent attention has been given to estimate pet image from its corresponding mri data. eliminating pet acquisition reduces time, costs and side effects to patients. related work. to date, a number of computational methods have been proposed for medical image synthesis using machine learning approaches. several researches employed conventional machine learning methods, such as gaussian arxiv:2012.15397v1 [eess.iv] 31 dec 2020 2 hajar emami, qiong liu, ming dong mixture model (gmm), structure random forest (srf), etc to generate missing type of medical image modality from available image modalities. in [1], the authors employed a regression forest for predicting the brain pet images using mri inputs. huynh et al. [2] used structured random forest together with an auto-context model to estimate ct patches from their corresponding mri patches. at the end, all generated ct patches from a given mri input are combined together to generate the corresponding ct image. recently, many deep learning models have been successfully applied to learning the mappings between two different image domains [3,4,5,6,7,8,9]. these deep learning approaches have been employed in medical imaging for generating missing type of modality from available type of imaging modality, e.g., generating synthetic ct images from mr images [10,11,12,13] or estimating pet images from mri data [14,15,16]. deep convolutional neural networks (cnns), a type of multi-layer deep learning models can be used to capture nonlinear mappings between input and output image domains. the cnn models are trained by minimizing the voxel-wise differences with respect to the generated output images and the ground truth data. cnn models were employed to perform image-to-image mapping between mri and ct images in [10]. li [14] developed a deep learning approach in which 3d-cnn is employed to estimate the output pet images given the input mri modality. a u-net architecture, a well-established fully convolutional network (fcn) first introduced in [17] for biomedical image segmentation. in [11], han developed a learning-based approach in which u-net architecture is employed to generate synthetic ct images from t1-weighted mr data. generative adversarial networks (gans) [18], another type of deep learning models with two competing networks: a generator to synthesize output images and a discriminator to distinguish between the synthesized and real data have been used in image generation tasks. recently, gans have been used in medical image-to-image translation tasks [19,16,15,20,12,21]. pan [15] used a gan model to capture the underlying relationship between mri and pet images to generate missing pet data. in the image-to-image translation task, the models learn the mapping between two image domains using a training set of paired data. since paired training data are not available in many tasks, zhu in [8] intoduced cyclegan to translate an image from a source domain to a target domain in the absence of paired data. unsupervised image to image translation using cyclegan was used in [22] to learn the mapping between unpaired mri and ct imaging modality. attention plays an important role in human visual process for building a visual representation and possessing context. inspired from human attention mechanism [23], attention-based deep learning models have been used in a variety of computer vision and machine learning tasks including image classification [24,25], image segmentation [26], image-to-image translation [27,28,29], natural language processing [30,31] and time series forecasting [32,33]. attention can be defined as a scalar matrix showing the relative importance of activation maps at different spatial locations [34]. incorporating attention into deep learning models frea-unet: frequency-aware u-net for modality transfer 3 helps to improves the performance by focusing on the most relevant features. zhou et al. [25] have shown that attention maps can be used to localize the object of interest and improve object localization accuracy. in [25], the attention in classification task is produced by removing top average-pooling layer of a classifier cnn and helped to identify the discriminative features across classes. zagoruyko et al. [35] improved the performance of a small student cnn network by transferring the attention knowledge from a larger and more powerful teacher cnn. they defined the attention based on the assumption that the absolute value of a neuron activation is relative to the importance of that neuron in order to classify the input image in classification task. jetley et al. [36] incorporated attention mechanism into a classifier cnn to amplify the relevant features and suppress the irrelevant features for classification of the input. their proposed approach bootstrapped standard cnn architectures in classification. recent studies show that incorporation of attention learning in both image generation and image-to-image translation tasks leads to more realistic output. zhang et al. [37] proposed self-attention gan that uses a selfattention mechanism in image generation task. chen et al. and mejjati et al. [28,27] used an attention network to localize the object of interest and excluding the background in image-to-image translation task to improve the quality of generated images. this work. while deep learning models are great potential in medical imaging, generating synthetic pet images from mri data is a challenging task due to their different appearances. in fig. 1, two examples of brain mr images (the first column) are shown with the corresponding pet images of the same patient (the second column) with a clear different textures. while mr images show richer texture information than pet images, pet images contain complicated texture with different frequency scales. this would impact synthetic pet generation, suggesting that separately optimizing on different frequency scales can lead to more realistic output with more preserved details in pet image generation. in this paper, we propose a novel deep learning method, a frequency-aware unet model (frea-unet), that generates low/high frequency scales of the image separately in different layers for estimating synthetic pet images. optimizing low/high frequency scales of the image with separate loss functions can help to balance between the error of the predicted pet image and the image sharpness and resolution. we also use end-to-end-trainable attention modules in our frea-unet to weight different features in low/high frequency scales. specifically, the attention in the low/high frequency layers is defined as the compatibility scores [36] between feature maps extracted at frequency layers and the output from the second to the last layer in u-net’s decoding part showing the relevant features for generating the output image. the extracted spatial attention scores are used in the scale layers so that higher weights are given to the relevant regions, leading to more realistic output images. the major contribution of our work is summarized as follows: 4 hajar emami, qiong liu, ming dong fig. 1. examples of brain mri datasets (the first column) with the corresponding pet images of the same patient (the second column), highlighting significant difference in mri and pet texture. – we proposed a novel frequency-aware u-net model for medical imaging modality transfer, frea-unet. based on the proposed model, we use a modified loss function with different weights to optimize different scales during frea-unet training. – we also incorporated attention mechanism into low/high frequency layers to focus more on the relevant features during image translation. – we proposed a complete end-to-end pet image generation model from mri data. – frea-unet demonstrates the effectiveness of separating low/high frequency generation and also incorporating the attention mechanism into the image generation model. through extensive experiments, we show that, both qualitatively and quantitatively, frea-unet significantly outperforms other state-of-the-art methods on adni dataset, offering potential pet/mri applications. 2 materials and methods 2.1 frea-unet the goal of the proposed frea-unet model is to estimate a mapping fmr→p et from the source image domain (mri) to the target image domain (pet). the mapping f is learned from paired training data s = {(mri , peti)|mri ∈ mr, peti ∈ p et, i = 1, 2, ..., n}, where n is the number of mri-pet image pairs. generating low/high frequency scales of the image separately with different loss functions can help the model to generate more realistic output with more preserved frea-unet: frequency-aware u-net for modality transfer 5 fig. 2. frea-unet architecture. low and high frequency pet scales are generated in two different layers of frea-unet’s decoding path. each frequency scale generation stream has one attention module to weight different features. the outputs of two frequency scale streams are fused and fed into last layer of the decoder to generate the final output pet image. low/high frequency details. the proposed frea-unet model achieves this by explicitly generating low and high frequency pet scales in two different layers of the u-net’s decoder as pictured in fig. 2. there are two end-to-end-trainable attention modules for low/high frequency layers to weight different features based on their importance in pet image generation. incorporating the attention mechanism into the layers responsible for generating low/high frequency scales helps the frea-unet model to focus more on the more relevant features during image translation leading to more realistic output images. 2.2 frequency-aware network u-net architecture has two symmetrical paths: an encoding path and a decoding path to leverage both local and hierarchical information in order to estimate more accurate reconstruction of the encoded input. similar to a regular cnn, the encoding path of the u-net contains convolutional layers to encode the context of the input image. on the other hand, the decoding path includes transposed convolutional layers to reconstruct the estimated image. the u-net architecture has direct connections across its contracting path and expanding path. these skip connections combine high resolution features from the contracting path with the up-sampled features from the expanding path and use them as the inputs to the convolutional layers in the expanding path. this typically leads to improved resolution for the network output. in our proposed frea-unet’s contracting path, we have six convolutional layers with 64, 128, 256, 512, 512, 512 filters, respectively. each convolutional 6 hajar emami, qiong liu, ming dong layer uses these filters to perform 2d convolutions on its input. the input of each layer is the output of its previous layer, except for the first layer in which the input is mri image. the convolution outputs are followed by a nonlinear activation function: rectified linear unit (relu), and batch normalization. batch normalization allows using much higher learning rates and accelerates the network training. in the expanding path of frea-unet architecture, we have six transposed convolutional layers with 512, 1024, 1024, 512, 256, 128 filters, respectively, followed by relu and batch normalization. in addition, we use dropout in layers of the frea-unet as an effective technique for regularization and preventing the co-adaptation of neurons in the network. in the proposed frea-unet architecture, we assign two different layers in the decoding path for low/high frequency image generation in order to generating low/high frequency pet images separately. specifically, we consider the fourth layer of the decoding path for low frequency generation and the fifth layer of the decoding path for high frequency pet image generation. low/high frequency scales are separated using a gaussian filter for each image during the training phase so the model can optimize different scales separately. as it is illustrated in fig. 2, low/high frequency scale outputs are combined after generation. as shown in our experimental results in section 3, optimizing on different scales helps generate more realistic pet images with improvement in quantitative results. learning the end-to-end mapping function from mri input and low/high frequency scales requires the estimation of network parameters achieved through minimizing the prediction error between the predicted images f(mr; θ) and the corresponding ground truth pet images. the objective for the mapping fmr→p etlow can be defied as: llow(f) = ||f(mr; θlow) − petlow||2 (1) similarly, the objective for updating the mapping fmr→p ethigh in high frequency stream of the generator is defined as: lhigh(f) = ||f(mr; θhigh) − pethigh||1 (2) since l1 norm leads to less blurry results in image generation tasks [5], we use l1 norm in our high scale generation stream. finally, by combining low/high frequency streams loss functions, the full objective of the frea-unet model can be expressed as: lf rea−unet = llow(f) + lhigh(f) (3) 2.3 attention module in the proposed frea-unet model, low/high frequency scales are optimized separately. we incorporate attention mechanism into low/high scale layers to weight different features based on their importance in the output pet image. specifically, the attention in the low/high frequency layers is defined as the compatibility scores [36] between feature maps extracted at frequency layers frea-unet: frequency-aware u-net for modality transfer 7 and the output from the second to the last layer in frea-unet’s decoding path showing the relevant features for generating the output image. we define the compatibility score as result of the dot product between the feature map fi and the output of the second to last layer of the model that has the input image size, having only to pass through the final layer to produce the output pet image opet: ci = hfi , opeti, i ∈ {1....n} (4) the extracted spatial attention scores are used in the low/high scale layers so that higher weights are given to the relevant regions, leading to more realistic output images. 2.4 datasets the brain dataset was acquired from 30 subjects with both mri and pet scans in the alzheimer’s disease neuroimaging initiative (adni) database (www. adni-info.org). adni provides a large database of studies with the goal of understanding the development and pathology of alzheimer’s disease. subjects are diagnosed as cognitively normal (cn), significant memory concern (smc), early mild cognitive impairment (emci), mild cognitive impairment (mci), late mild cognitive impairment (lmci) or having alzheimer’s disease (ad). all 30 pet subjects obtained from adni datasets were aligned to their corresponding t1- weighted mri using coregistration so there is spatial correspondence mri and pet slices in each subject. we cropped input mri and pet images to the size of 256×256 for training. 3 experimental results 3.1 experimental setup extensive experiments were performed to compare frea-unet with current sate-of-the-art models for generating synthetic pet images from mri inputs. based on the same training and testing splits, the following models used previously for similar image generation tasks are included in our empirical evaluation and comparison. – u-net model: following techniques developed by han [11], we implemented and trained u-net model to compare against the frea-unet approach. the u-net model used in our empirical evaluation has no attention mechanism and generates only one frequency scale output. the architecture includes six convolutional layers in the encoding path and six transposed convolutional layers in the decoding path. – pix2pix: we implemented and trained pix2pix model developed by isola [5], with five residual blocks in the generator. the discriminator is a regular cnn with five convolutional layers that classified the input image as real or synthetic. 8 hajar emami, qiong liu, ming dong – cgans with u-net architecture (ucgan): we implemented and trained conditional gan (cgan) model with u-net [17] architecture as its generator. we include comparison with ucgan as frea-unet’s core architecture is also based on the u-net. the discriminator is a regular cnn with five convolutional layers. 3.2 training and implementation details to evaluate the performance, three-fold cross-validation [38] was used in our model training and testing. that is, out of the 30 cases, 20 cases are randomly selected for the training, and the remaining cases are used to test the trained model. we repeat the experiment three times and report the averaged outcome. the weights in frea-unet were all initialized from a gaussian distribution with parameter values of 0 and 0.02 for mean and standard deviation, respectively. the model is trained with adam optimizer with an initial learning rate of 0.0002 and with a batch size of 1. we trained the model for 200 epochs on a nvidia gtx 1080 ti gpu. we used the same training setting for all models in our comparison. 3.3 quantitative measurement we used three commonly-used quantitative measures to evaluate the prediction accuracy of the models between the generated pet images and the real pet. these include mean absolute error (mae) defined below: mae = ph i=1 |realp et(i) − synp et(i) h (5) where h is the number of body voxels. the second measure is the structural similarity index (ssim) defined as: ssim = (2µp et µsynp et + c1)(2σp et synp et + c2) (µ 2 p et + µ 2 synp et + c1)(σ 2 p et + σ 2 synp et + c2) (6) where µp et denotes the mean value of pet image, µsynp et denotes the mean value of generated pet image, σ 2 p et is the variance of image pet, σ 2 synp et is the variance of image synpet, and the parameters c1 = (k1q) 2 and c2 = (k2q) 2 are two variables to stabilize the division with weak denominators, where k1 = 0.01 and k2 = 0.02. the last measure is the peak signal-to-noise ratio (psnr) defined as: p snr = 10log10( q2 mse ) (7) where q is the maximal intensity value of pet and synpet images, and mse is the mean squared error. for the mae metric, the lower value means better prediction results, that is, more realistic generated pet images. for both ssim and psnr, higher values indicate better prediction. frea-unet: frequency-aware u-net for modality transfer 9 3.4 ablation study we first performed model ablation to evaluate the impact of each component of frea-unet. in table 1, we report mae, psnr and ssim for different configurations of our model. first, we removed the attention mechanism from the model and also optimized the model on the original input scale (without seperating low/high frequency scales). as a consequence, we also used the regular u-net loss. in this case, our model is reduced to the u-net architecture (u-net) with mae: 52.56 ± 6.71, psnr: 26.75 ± 1.96 and ssim: 0.84 ± 0.05. next, we removed ”optimization on different scales” but kept attention mechanism to weight different features (frea-unet-wo-freq). our results show that this leads to better results (mae: 51.10 ± 7.03, psnr: 27.08 ± 1.33 and ssim: 0.85 ± 0.04) than u-net, but worse than frea-unet. clearly, separately optimizing low/high scales can help to achieve more accurate synpets with preserved organ structures. further, we removed the attention mechanism from our model and optimized the model on low/high scales separately (frea-unet-wo-att). this results to a higher mae (48.19 ± 6.42) and lower psnr (27.93 ± 1.84) and ssim (0.86 ± 0.04) when compared with the full version of frea-unet. this ablation study showed that frea-unet works better when attention is employed to weight different features based on their importance in estimating pet images. separately optimizing low/high scales using different weights obtained from attention modules helped to generate more realistic synthetic pets with higher spatial resolution. table 1. performance comparison for ablations of frea-unet in synpet generation. mae psnr ssim u-net 52.56 ± 6.71 26.75 ± 1.96 0.84 ± 0.05 frea-unet-wo-freq 51.10 ± 7.03 27.08 ± 1.33 0.85 ± 0.04 frea-unet-wo-att 48.19 ± 6.42 27.93 ± 1.84 0.86 ± 0.04 frea-unet 46.26 ± 5.03 28.45 ± 2.01 0.87 ± 0.04 3.5 performance of image synthesis models fig. 3, shows generated synpet results using different models for different test cases. the images in the columns are the input mri, real pet, and the generated pets using the proposed frea-unet model, u-net [11], ucgan [17], and pix2pix [5] respectively. qualitative comparisons for different methods show that the synthetic pets generated by the proposed frea-unet are more accurate in estimating anatomical structures and have more preserved details compared to synpets generated by other models. since frea-unet generates low/high frequency scales separately, its generated results have higher spatial resolution, and have a successful translation in generating sharp regions including boundaries as shown in fig. 3. 10 hajar emami, qiong liu, ming dong fig. 3. qualitative comparison of synthetic pets generated with frea-unet and state-of-the-art models for different patients. the input mri, corresponding real pet, synpet generated by frea-unet, u-net, ucgan and pix2pix, are shown in the columns, respectively for different patients. table 2. performance comparison of the frea-unet with other models for synthetic pet generation. for each model the average maes are computed from entire head. the average psnr and ssim are also reported in the second and third rows, respectively. method mae psnr ssim u-net 52.56 ± 6.71 26.75 ± 1.96 0.84 ± 0.04 pix2pix 59.19 ± 7.89 24.86 ± 2.34 0.79 ± 0.04 ucgan 55.31 ± 6.25 26.11 ± 2.21 0.82 ± 0.05 frea-unet 46.26 ± 5.03 28.45 ± 2.01 0.87 ± 0.04 the average mae, psnr, and ssim metrics computed based on the real and synthetic pets for all test cases are listed in table 2 for each of the aforementioned methods. the frea-unet achieved the best performance in synthetic frea-unet: frequency-aware u-net for modality transfer 11 pet generation compared to other models with the average mae of 46.26±5.03 for all test cases. 3.6 conclusions in this paper, we propose a frequency-aware u-net model, frea-unet, for modality transformation in medical imaging. frea-unet optimizes on different scales separately and takes advantage of attention mechanism to weight different features in low/high scale generation. separately optimizing low/high scales using different weights obtained from attention modules helped to generate more accurate synthetic pets with preserved organ structures. we have applied this model to estimate pet images from their corresponding mr images on adni dataset where we are able to generate sharp synthetic pet images similar to real pet images. our experiments show that the proposed frea-unet model can outperform other state-of-the-art methods in modality transfer of medical imaging. references 1. kang, j., gao, y., shi, f., lalush, d.s., lin, w., shen, d.: prediction of standarddose brain pet image by using mri and low-dose brain [18f] fdg pet images. medical physics 42 (2015) 5301–5309 2. huynh, t., gao, y., kang, j., wang, l., zhang, p., lian, j., shen, d.: estimating ct image from mri data using structured random forest and auto-context model. ieee transactions on medical imaging 35 (2015) 174–183 3. choi, y., choi, m., kim, m., ha, j.w., kim, s., choo, j.: stargan: unified generative adversarial networks for multi-domain image-to-image translation. in: proceedings of the ieee conference on computer vision and pattern recognition. (2018) 8789–8797 4. huang, x., liu, m.y., belongie, s., kautz, j.: multimodal unsupervised imageto-image translation. in: proceedings of the european conference on computer vision (eccv). (2018) 172–189 5. isola, p., zhu, j.y., zhou, t., efros, a.a.: image-to-image translation with conditional adversarial networks. in: proceedings of the ieee conference on computer vision and pattern recognition. (2017) 1125–1134 6. liu, m.y., breuel, t., kautz, j.: unsupervised image-to-image translation networks. in: advances in neural information processing systems. (2017) 700–708 7. murez, z., kolouri, s., kriegman, d., ramamoorthi, r., kim, k.: image to image translation for domain adaptation. in: proceedings of the ieee conference on computer vision and pattern recognition. (2018) 4500–4509 8. zhu, j.y., park, t., isola, p., efros, a.a.: unpaired image-to-image translation using cycle-consistent adversarial networks. in: proceedings of the ieee international conference on computer vision. (2017) 2223–2232 9. zhu, j.y., zhang, r., pathak, d., darrell, t., efros, a.a., wang, o., shechtman, e.: toward multimodal image-to-image translation. in: advances in neural information processing systems. (2017) 465–476 12 hajar emami, qiong liu, ming dong 10. nie, d., cao, x., gao, y., wang, l., shen, d.: estimating ct image from mri data using 3d fully convolutional networks. in: deep learning and data labeling for medical applications. springer (2016) 170–178 11. han, x.: mr-based synthetic ct generation using a deep convolutional neural network method. medical physics 44 (2017) 1408–1419 12. emami, h., dong, m., nejad-davarani, s.p., glide-hurst, c.k.: generating synthetic cts from magnetic resonance images using generative adversarial networks. medical physics 45 (2018) 3627–3636 13. emami, h., dong, m., glide-hurst, c.k.: attention-guided generative adversarial network to address atypical anatomy in modality transfer. arxiv preprint arxiv:2006.15264 (2020) 14. li, r., zhang, w., suk, h.i., wang, l., li, j., shen, d., ji, s.: deep learning based imaging data completion for improved brain disease diagnosis. in: international conference on medical image computing and computer-assisted intervention, springer (2014) 305–312 15. pan, y., liu, m., lian, c., zhou, t., xia, y., shen, d.: synthesizing missing pet from mri with cycle-consistent generative adversarial networks for alzheimer’s disease diagnosis. in: international conference on medical image computing and computer-assisted intervention, springer (2018) 455–463 16. armanious, k., jiang, c., fischer, m., k¨ustner, t., hepp, t., nikolaou, k., gatidis, s., yang, b.: medgan: medical image translation using gans. computerized medical imaging and graphics 79 (2020) 101684 17. ronneberger, o., fischer, p., brox, t.: u-net: convolutional networks for biomedical image segmentation. in: international conference on medical image computing and computer-assisted intervention, springer (2015) 234–241 18. goodfellow, i., pouget-abadie, j., mirza, m., xu, b., warde-farley, d., ozair, s., courville, a., bengio, y.: generative adversarial nets. in: advances in neural information processing systems. (2014) 2672–2680 19. gehari, h.e., nejad-davarani, s., dong, m., glide-hurst, c.: generating synthetic cts from magnetic resonance images using generative adversarial networks. in: medical physics. volume 45., wiley 111 river st, hoboken 07030-5774, nj usa (2018) e131–e131 20. nie, d., trullo, r., lian, j., petitjean, c., ruan, s., wang, q., shen, d.: medical image synthesis with context-aware generative adversarial networks. in: international conference on medical image computing and computer-assisted intervention, springer (2017) 417–425 21. emami, h., dong, m., glide-hurst, c.k.: attention-guided generative adversarial network to address atypical anatomy in synthetic ct generation. in: 2020 ieee 21st international conference on information reuse and integration for data science (iri), ieee (2020) 188–193 22. wolterink, j.m., dinkla, a.m., savenije, m.h., seevinck, p.r., van den berg, c.a., iˇsgum, i.: deep mr to ct synthesis using unpaired data. in: international workshop on simulation and synthesis in medical imaging, springer (2017) 14–23 23. rensink, r.a.: the dynamic representation of scenes. visual cognition 7 (2000) 17–42 24. xiao, t., xu, y., yang, k., zhang, j., peng, y., zhang, z.: the application of two-level attention models in deep convolutional neural network for fine-grained image classification. in: proceedings of the ieee conference on computer vision and pattern recognition. (2015) 842–850 frea-unet: frequency-aware u-net for modality transfer 13 25. zhou, b., khosla, a., lapedriza, a., oliva, a., torralba, a.: learning deep features for discriminative localization. in: proceedings of the ieee conference on computer vision and pattern recognition. (2016) 2921–2929 26. chen, l.c., yang, y., wang, j., xu, w., yuille, a.l.: attention to scale: scaleaware semantic image segmentation. in: proceedings of the ieee conference on computer vision and pattern recognition. (2016) 3640–3649 27. mejjati, y.a., richardt, c., tompkin, j., cosker, d., kim, k.i.: unsupervised attention-guided image-to-image translation. in: advances in neural information processing systems. (2018) 3693–3703 28. chen, x., xu, c., yang, x., tao, d.: attention-gan for object transfiguration in wild images. in: proceedings of the european conference on computer vision (eccv). (2018) 164–180 29. emami, h., aliabadi, m.m., dong, m., chinnam, r.: spa-gan: spatial attention gan for image-to-image translation. ieee transactions on multimedia (2020) 30. eriguchi, a., hashimoto, k., tsuruoka, y.: tree-to-sequence attentional neural machine translation. arxiv preprint arxiv:1603.06075 (2016) 31. lin, z., feng, m., santos, c.n.d., yu, m., xiang, b., zhou, b., bengio, y.: a structured self-attentive sentence embedding. arxiv preprint arxiv:1703.03130 (2017) 32. siridhipakul, c., vateekul, p.: multi-step power consumption forecasting in thailand using dual-stage attentional lstm. in: 2019 11th international conference on information technology and electrical engineering (icitee), ieee (2019) 1–6 33. aliabadi, m.m., emami, h., dong, m., huang, y.: attention-based recurrent neural network for multistep-ahead prediction of process performance. computers & chemical engineering (2020) 106931 34. simonyan, k., vedaldi, a., zisserman, a.: deep inside convolutional networks: visualising image classification models and saliency maps. arxiv preprint arxiv:1312.6034 (2013) 35. zagoruyko, s., komodakis, n.: paying more attention to attention: improving the performance of convolutional neural networks via attention transfer. arxiv preprint arxiv:1612.03928 (2016) 36. jetley, s., lord, n.a., lee, n., torr, p.h.: learn to pay attention. arxiv preprint arxiv:1804.02391 (2018) 37. zhang, h., goodfellow, i., metaxas, d., odena, a.: self-attention generative adversarial networks. arxiv preprint arxiv:1805.08318 (2018) 38. stone, m.: cross-validatory choice and assessment of statistical predictions. journal of the royal statistical society: series b (methodological) 36 (1974) 111–133. noname manuscript no. (will be inserted by the editor) calibration of biophysical models for tau-protein spreading in alzheimer’s disease from pet-mri klaudius scheufele · shashank subramanian · george biros received: date / accepted: date abstract aggregates of misfolded tau proteins (or just “tau” for brevity) play a crucial role in the progression of alzheimer’s disease (ad) as they correlate with cell death and accelerated tissue atrophy. longitudinal positron emission tomography (pet) scans can be used quantify the extend of abnormal tau spread. such pet-based image biomarkers are a promising technology for ad diagnosis and prognosis. here, we propose to calibrate an organ-scale biophysical mathematical model using longitudinal pet scans to extract characteristic growth patterns and spreading of tau. the biophysical model is a reaction-advection-diffusion partial differential equation (pde) with only two scalar unknown parameters, one representing the spreading (the diffusion part of the pde) and the other one the growth of tau (the reaction part of the pde). the advection term captures tissue atrophy and is obtained from diffeomorphic registration of longitudinal magnetic resonance imaging (mri) scans. we describe the method, present a numerical scheme for the calibration of the growth and spreading parameters, perform a sensitivity study using synthetic data, and we perform a preliminary evaluation on clinical scans from the adni dataset. we study whether such model calibration is possible and investigate the sensitivity of such calibration to the time between consecutive scans and the presence of atrophy. our findings show that despite using only two calibration parameters, the model can reconstruct clinical scans quite accurately. we discovered that small time k. scheufele oden institute, university of texas at austin, texas, e-mail: klaudius.scheufele@gmail.com s. subramanian oden institute, university of texas at austin, texas, e-mail: shashanksubramanian@utmail.utexas.edu g. biros oden institute, university of texas at austin, texas, e-mail: biros@oden.utexas.edu arxiv:2007.01236v2 [q-bio.qm] 5 dec 2020 2 klaudius scheufele et al. intervals between scans and the presence of background noise create difficulties. furthermore, we evaluate the sensitivity of the model calibration to the offset considered for background noise subtraction. our reconstructed model fits the data well, yet the study on clinical data also reveals shortcomings of the simplistic model. interestingly, the values of the model parameters have significant variability across patients, an indication that these parameters could be useful biomarkers. keywords alzheimer’s disease · prion spreading · model personalization. 1 introduction alzheimer’s disease is the sixth highest leading cause of death in the us (alz.org). its complex evolution is thought to be closely related to the formation and spreading of abnormal proteinaceous assemblies in the nervous system. in particular, the toxic misfolding of β-amyloid (aβ) and tau proteins are believed to be key factors driving the progression of alzheimer’s disease [20,21,38]. these corruptive protein templates incite a chain reaction of misfolding, by inducing their anomalous structure on benign molecules. subsequent growth, fragmentation and further spreading of such toxic proteins hampers proper function of the nervous system, leads to accelerated tissue atrophy, necrosis, and ultimately causes death [5, 22, 32, 35]. tau aggregates are primarily found in the axon bundle and rapidly spread along neuronal pathways to distant locations, but also invade the extracellular space [19]. understanding the distinct spatiotemporal growth patterns and original seeding of corrupted protein templates is imperative in developing new treatment protocols and can reveal important complimentary information to aid diagnosis and overall efficacy of therapy. longitudinal pet scans—using the f-av-1451 tracer, also referred to as tauvid1—can image tau spreading and lead to improved diagnosis and prognosis. many groups are designing image-analysis algorithms for this purpose [14,25,36,37]. here we propose a complementary approach: we employ a pde model of tau propagation and calibrate its parameters using longitudinal pet scans. in clinical practice, tau-pet is acquired in intervals of 8 to 15 months with varying, but typically rather small relative change of measured tau uptake (say between 10% and 20%). our goal is to estimate the rate of amplification of misfolded tau-protein aggregates and the rate of fragmentation and migration thereof to distant parts of the brain. in particular, we study the sensitivity of such calibration to small relative changes in signal and tissue atrophy. our hypothesis is that an informative minimal model can produce biomarkers, which can augment imaged-based approaches. once calibrated, the current and future spatiotemporal spreading of tau can be quantified using our model. 1 http://pi.lilly.com/us/tauvid-uspi.pdf calibration of biophysical models for tau spreading in ad 3 contributions. we study the image-driven calibration of a biophysical model for spatiotemporal evolution of tau protein misfolding. specifically, we investigate the effect of the time horizon between consecutive scans, and whether accounting for tissue atrophy affects the reconstruction of tau parameters. lastly, we study how our method applies to real data and how imaging noise can be handled. the novelty of our work can be summarized as follows: 1) we formulate and solve an inverse problem to estimate patient specific, characteristic growth parameters describing the spatiotemporal evolution of misfolded tau-protein throughout the brain based on longitudinal 3d tau-pet imaging of ad subjects. we model tau progression as a reactiondiffusion-advection pde, which accounts for tau propagation and takes into account observed atrophy. the atrophy is modeled using material transport with a velocity obtained from diffeomorphic image registration, and is oneway coupled to tau progression. 2) we investigate and demonstrate the ability to accurately estimate the model’s growth parameters. we perform a sensitivity study with respect to the relative change in tau signal between scans, and the effect of tissue atrophy. for this experiment, we use synthetic data. our results indicate good agreement for future prediction of tau uptake compared to the true data. 3) we test our method on clinical tau-pet scans from the alzheimer’s disease neuroimaging initiative (adni)2 database, and study the sensitivity of our model to different subjects, and algorithmic hyper-parameters. to our knowledge, this is the first study of its kind. related work. biophysical modeling for ad started off with the advent of the prion-paradigm (misfolding chain-reaction) disease model. models range from molecular level [6, 15], to graph models [1, 7, 12, 33], and kinetic equations [3]. inspired by the successful application of such models in computational oncology [2, 10, 24, 42], we use an organ level pde-model which reflects the most dominant evolution patterns of tau propagation. for ad, a similar model to the one we propose here has been recently proposed for prion-like diseases [40, 41]. to the best of our knowledge, our work here is the first to study image-driven calibration of a biophysical model for tau propagation. the remainder of the paper is structured as follows: §2 discusses the physics-based model for the propagation of tau-protein, followed by the formulation and numerical solution of the inverse problem for image-driven calibration. in §3, we investigate the invertability of growth parameters, perform a sensitivity study, and apply our method to clinical data. we conclude the paper with a discussion in §4. 2 http://adni.loni.usc.edu/ 4 klaudius scheufele et al. 2 methods 2.1 models and materials forward model. to model the spatiotemporal evolution of misfolded tauprotein, we adopt the fisher-kolmogorov equation [31] coupled with an advection equation for material transport: ∂tc + ∇ · (cv) − κdc − ρrc = 0, in Ωb × (0, t], (1a) c(0) = c0 in Ωb, (1b) ∂tm + ∇mv = 0, in Ω × (0, t], (1c) m(0) = m0 in Ω. (1d) this simple model captures the basic dynamics of the problem: (i) the growth and formation of tau aggregates, and (ii) their subsequent fragmentation and spatial propagation. it further represents ad specific characteristics such as slow early stage progression with a rapid acceleration after symptom onset, and the inevitable progression of the disease: even a single corruptive protein will spread, and ultimately cause disease [30]. our model follows [40, 41], but differs in a new image-driven transport term, that captures atrophy without a mechanical model. in our case c = c(x, t) ∈ [0, 1] is the tau concentration with initial seeding c0. its evolution over time t ∈ [0, t] in the three-dimensional brain domain Ωb ⊂ Ω = [0, 2π] 3 is governed by the coefficients of the reaction and diffusion terms in (1a): the nonlinearity rc = ρmc(1 − c) with growth rate ρ provides a saturation term expressing the maximal concentration of toxic proteins. ρm is a spatially variable coefficient, depended on the underlying material properties m = (mi(x, t))i=w, g, f , a vector of voxel-wise probabilities for white matter (w), gray matter (g), and cerebrospinal fluid with ventricles (f). spatial spreading of tau is driven by extracellular diffusion and axonal transport. this is modeled by the diffusion operator dc = ∇ · κm∇c, where κm = κ0 i + (κi/κ − 1)t defines the inhomogeneous (anisotropic) diffusion tensor. κ0 captures the inhomogeneous diffusion based on the material properties m, while t expresses preferential direction of diffusion along the axon bundle weighted by κi , and enables anisotropy. tau does not invade the cerebrospinal fluid, and spreading occurs primarily in white matter [11]. we approximate no-flux boundary conditions ∂Ωb via a penalty approach [8], and use periodic boundary conditions on ∂Ω. atrophy representation. over time, tau aggregates disrupt cell function and ultimately cause cell death and tissue atrophy (thinning of white and gray matter) [22]. we represent tissue atrophy as image-driven transport equation (1c)–(1d) of the spatiotemporal material properties m. as a result, tissue atrophy is one-way coupled to tau spreading via the definition of the spatially variable diffusion and reaction coefficients. calibration of biophysical models for tau spreading in ad 5 the velocity field v is found via large-deformation diffeomorphic image registration of longitudinal mri data, and can be computed solving the inverse problem min ~v 1 2 km~ (1) − m~ tk 2 l2(Ω) + β 2 s(v) s.t. (1c)–(1d). (2) here, m~ 0 and m~ t denote segmentations of mri scans, corresponding to the acquisition times t = 0 and t = t of the tau-pet time-series. the regularization term s(v) is given as an h1 -seminorm r Ω ∑ 3 i−1 |∇v i (x)| 2 dx. for the solution of (2) we use the registration software claire [29]. the proposed atrophy model is distinct from other approaches in the literature. in [40, 41] the authors couple a reaction-diffusion equation to a mechanical deformation model for atrophy, based on nonlinear elasticity. solving (1) defines the forward problem f(c0, ct,~v, ρ, κ) = 0. next, we discuss the image-driven calibration of this model. 2.2 model calibration and extraction of tau-spreading characteristics the model is personalized based on patient specific, tau-pet imaging data. that is, we estimate biophysical growth parameters g = (ρ, κ) in (1) based on measurements of tau uptake value ratios (suvr) from longitudinal pet by solving a parameter estimation problem min g=(ρ,κ) 1 2 ko(ct − dt)k 2 l2(Ω) s.t. f(c0, ct,~v, ρ, κ) = 0, from (1), (3) minimizing the discrepancy between predicted tau concentration ct = c(t) (based on the seeding c(0)) and the target data dt. the observation operator o allows to specify a ‘valid‘ region (i.e., region of high signal confidence/relevance) of the tau-pet scan, which drives the parameter inversion. although there has been success in reducing non-specific tracer binding in white matter, it remains to be problematic and white matter regions are disregarded when interpreting tau-pet scans [26, 39]. following this practice, we restrict the data term to gray matter only, and define oc := 1[x∈gm] c. the seeding concentration c(0) is defined by the data d0 from the first time point d0 =: c0. as a proof of concept, we only consider isotropic diffusion for the model calibration. the extension to anisotropy is straightforward and will be realized in future work. before, discussing the numerical scheme for the solution of (3), we want to discuss some characteristics of the parameter inversion using a simplified analytical model in one dimension. 2.3 parameter inversion analysis for linearized model although our model has only two free parameters, the inversion problem (3) may be ill-conditioned and suffer from strong sensitivity to perturbations. 6 klaudius scheufele et al. to demonstrate some of the inherent instabilities, we study the analytic solution for a simplified version of the forward model (1). we linearize around c(x, t) = 0, take v = 0, and consider constant coefficients in a onedimensional domain ω = [0, π] with periodic boundary conditions: ∂tc − κ∂xxc − ρc = 0 in ω × (0, t], (4a) c(0) = c0 in ω, ∂xc(0) = ∂xc(π) = 0. (4b) the bvp (4) can be solved analytically by separation of variables, yielding c(x, t) = ∞ ∑ n=0 cˆn exp (ρ − n 2 κ)t cos(nx), (5) where cˆn are the spectral cosine coefficients of the initial condition c0. assuming band-limited data dt = ∑ n n=0 ˆdn cos(nx), the parameter inversion problem for ρ, κ reads minρ,κ = 1 2 n ∑ n=0 cˆn exp (ρ − n 2 κ)t − ˆdn 2 . (6) it is easy to see that a minimum is found if cˆn exp (ρ − n 2 κ)t = ˆdn or 1 t log cˆn ˆdn = −ρ + n 2 κ. (7) from (7) we observe, that any numerical errors, noise, or model errors are expected to be amplified when the time horizon decreases. 2.4 numerical scheme numerical solution of the forward problem. we employ a pseudo-spectral fourier approach on a regular grid for spatial discretization. that means all spatial differential operators are computed via a 3d fast fourier transform [9]. for numerical solution of the forward problem, we employ a firstorder operator-splitting method to split the tau progression equation (1a) into a reaction, diffusion, and advection part. for the diffusion split, we use an implicit crank-nicholson method, and solve the linear system with a preconditioned matrix-free cg method. the reaction sub-steps are solved analytically. the hyperbolic transport equations are solved using an unconditionally stable semi-lagrangian time-stepping scheme to avoid stability issues and small, cfl restricted time-steps [27, 28]. this method requires evaluations of the space-time fields at off-grid locations, defined by the characteristic associated with v. we compute off-grid evaluations using a cubic lagrange polynomial interpolation model. we use nt = 100 time steps of size ∆t = 0.01 for the time integration. calibration of biophysical models for tau spreading in ad 7 numerical optimization. the optimization problem (3) is solved using a bound constrained, limited memory bfgs quasi-newton solver globalized with armijo line-search. the gradient is computed using a first-order accurate finite difference scheme with~h = √ emach~g, for ~g = (ρ, κ) t . we terminate the optimization after a gradient reduction of 4 orders of magnitude (relative to the initial guess). to keep the optimizer within feasible bounds, and prevent bad local minima, we define bound constraints κmin = 1e−4, and κmax = 1, ρmin = 0.1, and ρmax = 15.3 2.5 workflow summary for the evaluation on clinical data, we use seven ad subjects from the adni database as outlined in tab. 2. imaging data is processed using fsl [16] by applying the following steps: (i) longitudinal mri scans are rigidly registered to the first time point using flirt [18]; (ii) longitudinal tau-pet images are rigidly registered to the first time point mri; (iii) aligned images are skull-stripped using bet [17]; (iv) registered brain masks are applied in pet space to obtain the skull-stripped pet image; (v) skull-stripped images are segmented using fast [43]; (vi) pet images are individually normalized with average tau uptake value in the cerebellum. for the last step, the cerebellum is extracted by registering the adni subject image to labeled atlas. 3 results we examine the quality of biophysical model personalization, and the accuracy of subsequent prediction of tau spreading. we ask the following two questions: (i) how does the solution of (3) depend on the time horizon t and the effects of tissue atrophy (modeled via material transport)? and (ii) how does our method perform on clinical tau-pet scans? 3.1 accuracy of model calibration and tau forecast using synthetic data we study the sensitivity of model calibration and prediction to (i) small relative change in tau signal (varying time frames of image acquisition), and (ii) the effect of tissue atrophy for a synthetic setup. fig. 1 gives an illustration of tau propagation and the extend of atrophy. synthetic data. as baseline brain anatomy, we use (segmentations of) longitudinal mri scans of an ad subject with clearly visible tissue atrophy (cf. panel a in fig. 1). the velocity v, used to couple the effect of tissue 3 the lower bound for κi is determined by the smallest recoverable diffusivity considering a resolution of 2563 . 8 klaudius scheufele et al. atrophy to tau spreading via the advection term in (1), is obtained using diffeomorphic image registration between segmentations of the mri of the first and second scan. we generated synthetic tau data using our reactiondiffusion-advection model for tau evolution based on the tissue segmentation of the first time point, and ground truth parameters (proliferation rate of ρ = 8, and migration rate of κ = 0.18; nondimensionalized). synthetic data was generated using twice as many discretization points in time as compared to the inversion. an illustration is given in panel b of fig. 1. the primary purpose of these synthetic experiments is to set up a simple problem to study the effects of tissue atrophy and small relative change in signal. thus, the synthetic data is observed everywhere and observations are noise free. we consider three scenarios: i. disregard tissue atrophy and define material properties for tau-propagation based on structural mri at first time point t0; ii. repeat the same setup but now use the structural mri at time point t1; iii. account for tissue atrophy via material transport, governed by a velocity field obtained from image registration of (segmented) mri. for each scenario, the time horizon between image acquisition of consecutive scans is varied (the first snapshot d0 is taken at different times t0 = 0.0, 0.56, . . . , 0.99), with a relative change in tau uptake signal ranging from 99% to 3%. since image acquisition time intervals are rather short in practice (8-15 months), studying the sensitivity of the calibration to small relative changes in the signal, is of great interest. results are given in tab. 1. performance metrics. we report relative errors eι = ιrec/ι ? , ι ∈ {ρ, κ} in the reconstruction of the growth parameters. we further report the relative forecast errors µt = kc(t)−dtk2/kdtk2 for predicted tau concentration at future time points t ∈ {t 0 , t 00}. all numerical experiments are indicated with a unique identification number #xy (first column of tables). sensitivity to small relative change in tau signal. runs #17–#24 (scenario iii) in tab. 1 show the calibration results and subsequent tau forecast for varying acquisition times of the first scan d0. we observe a slight deterioration in reconstruction accuracy of the true model parameters for smaller time horizons between scans. for particularly small relative change in tau uptake, we see larger errors and a trend of under-estimating the growth parameters. up to 10% relative change in tau suv, relative errors, however, are still small with 5% and below for ρ, and up to 20% for κ. similarly the accuracy of tau forecast deteriorates marginally, but remains low with errors of 3-13% for t 0 , and 5-26% for t 00 (see also panel c in fig. 1). calibration of biophysical models for tau spreading in ad 9 table 1: model calibration and tau evolution forecasting for synthetic data. we study the sensitivity with respect to 1) the time horizon t between consecutive scans, and 2) tissue atrophy (one-way coupled to tau spreading via material transport). for 1), we consider varying image acquisition time points t0 for d0 with a relative change (∆ suv) in tau signal (between scans d0 and d1) ranging from 99% to 3%. for 2), we consider three different scenarios: i. disregard tissue atrophy and use structural mri of t0 as material properties for tau-propagation; ii. like before but use mri of t1; iii. account for tissue atrophy via material transport, governed by a velocity field found from image registration of (segmented) t1 and t0 mri. we report relative errors eρ and eκ for the inversion parameters (true values are ρ ? = 8, κ ? = 0.18). µt1 denotes the relative data misfit in the inversion; µt0 and µt00 denote forecast errors at future times t0 = 1.2, and t00 = 1.5. id t0 t1 ∆ suv ρ κ eρ eκ µt1 µt0 µt00 #1 i. t0 mri 0.00 1.00 99% 7.77 1.70e−1 2.91e−2 5.34e−2 3.37e−1 3.90e−1 4.42e−1 #2 0.56 1.00 80% 7.89 1.85e−1 1.37e−2 2.89e−2 3.03e−1 3.70e−1 4.29e−1 #3 0.72 1.00 60% 7.70 1.52e−1 3.70e−2 1.53e−1 2.56e−1 3.43e−1 4.18e−1 #4 0.83 1.00 40% 7.21 9.36e−2 9.93e−2 4.80e−1 1.91e−1 3.13e−1 4.17e−1 #5 0.92 1.00 20% 6.48 4.12e−2 1.90e−1 7.71e−1 1.12e−1 2.80e−1 4.27e−1 #6 0.96 1.00 10% 5.96 1.49e−2 2.55e−1 9.17e−1 6.03e−2 2.63e−1 4.39e−1 #7 0.98 1.00 5% 5.78 6.92e−3 2.77e−1 9.62e−1 3.09e−2 2.52e−1 4.43e−1 #8 0.99 1.00 3% 5.73 4.52e−3 2.84e−1 9.75e−1 1.56e−2 2.45e−1 4.43e−1 #9 ii. t1 mri 0.00 1.00 99% 7.53 1.59e−1 5.93e−2 1.18e−1 2.49e−1 2.66e−1 3.06e−1 #10 0.56 1.00 80% 7.71 1.65e−1 3.57e−2 8.40e−2 1.84e−1 2.22e−1 2.77e−1 #11 0.72 1.00 60% 7.70 1.56e−1 3.72e−2 1.36e−1 1.48e−1 2.01e−1 2.67e−1 #12 0.83 1.00 40% 7.58 1.37e−1 5.19e−2 2.41e−1 1.14e−1 1.83e−1 2.60e−1 #13 0.92 1.00 20% 7.28 1.07e−1 9.00e−2 4.06e−1 7.35e−2 1.66e−1 2.60e−1 #14 0.96 1.00 10% 6.83 7.46e−2 1.46e−1 5.86e−1 4.39e−2 1.61e−1 2.75e−1 #15 0.98 1.00 5% 6.31 4.14e−2 2.12e−1 7.70e−1 2.42e−2 1.69e−1 3.06e−1 #16 0.99 1.00 3% 6.02 2.36e−2 2.48e−1 8.69e−1 1.26e−2 1.77e−1 3.29e−1 #17 iii. adv., t0 mri 0.00 1.00 99% 7.90 1.80e−1 1.31e−2 4.72e−4 2.38e−2 3.34e−2 5.23e−2 #18 0.56 1.00 80% 7.90 1.83e−1 1.20e−2 1.71e−2 9.74e−2 1.02e−1 1.11e−1 #19 0.72 1.00 60% 7.89 1.81e−1 1.37e−2 4.21e−3 9.21e−2 1.09e−1 1.27e−1 #20 0.83 1.00 40% 7.83 1.72e−1 2.16e−2 4.65e−2 7.30e−2 1.06e−1 1.33e−1 #21 0.92 1.00 20% 7.70 1.57e−1 3.71e−2 1.25e−1 4.61e−2 9.90e−2 1.38e−1 #22 0.96 1.00 10% 7.56 1.43e−1 5.45e−2 2.05e−1 2.66e−2 9.42e−2 1.43e−1 #23 0.98 1.00 5% 6.66 7.22e−2 1.68e−1 5.99e−1 1.76e−2 1.22e−1 2.19e−1 #24 0.99 1.00 3% 5.97 6.73e−2 2.54e−1 6.26e−1 9.89e−3 1.37e−1 2.64e−1 10 klaudius scheufele et al. fig. 1: qualitative results for model calibration and tau evolution forecasting using synthetic data. panel a illustrates structural differences induced by tissue atrophy: a)–1 and a)–5 show original mri scans of an ad subject at time points t0 = 0 and t1 = 1; a)–2 through a)–4 show the change of the csf structure at different time points due to atrophy; a)–5 and 6 show the magnitude and deformation gradient of the velocity, obtained from registration of mri (a value of |∇ v| < 1 indicates contraction, a value above 1 expansion; the registration is performed from the second to the first snapshot). panel b shows synthetic tau data at different time points used for calibration and tau distribution at forecast time points t 0 and t00 (cf. tab. 1). panel c shows the forecast distribution c 0 and c00 of tau at time points t 0 and t00 based on calibration for the three scenarios i-iii from tab. 1 and two different time horizons, along with the relative mismatch to the ground truth (dark black indicates high error). calibration of biophysical models for tau spreading in ad 11 sensitivity to effects from tissue atrophy (material transport). next, we study the sensitivity of the model calibration due to atrophy-induced structural changes, that is the distribution of csf, gray and white matter. from tab. 1 we observe that the effect of tissue atrophy is not negligible for the calibration of our model: disregarding it misses the effects of cortical thinning, and this results in quite large errors for both scenarios i and ii, compared to the full model in scenario iii. most noticeably, smaller relative change of tau suv between scans results in a loss of accuracy for both the calibrated growth parameters and the forecast of tau concentration. this accuracy loss is more severe, if the advection of material properties, according to the cortical thinning, is disregarded. the differences in the forecast of tau distribution at future time points can be seen in fig. 1 (panel c) for the different scenarios. we conclude that the model is sensitive in the presence of large atrophy and the coupling of tissue atrophy, captured by the transport velocity v, is important for reconstructing the correct parameters. of course, this sensitivity depends on the extend of observable atrophy in the longitudinal mri scans. if the atrophy is negligible, then the advection term in the tau propagation term will be almost zero. 3.2 accuracy and model calibration on the adni clinical data clinical data. we use tau-pet scans from the adni database for calibration. the andi-tau study is ongoing and the dataset is not yet complete. for this exploratory study, we selected cases with clear longitudinal differences in the tau signal and at least one good quality mri. subjects with only one tau scan or non-increasing tau uptake were excluded. patient demographics are given in tab. 2. pet scans are acquired one to two years apart; the number of days between image acquisition is given by t. not all cases have mri for the second time point; we do not account for atrophy changes for these subjects. raw adni imaging data is processed as described in §2.5. pet imaging typically features a low signal-to-noise ratio, which complicates the calibration process. furthermore the 18f (tauvid) agent exhibits non-specific background activity of healthy tissue, and its dynamic range is at an offset. the tauvid imaging protocol4 suggests to correct for this ambient signal by subtraction of a constant-intensity ‘all black‘ image. the offset threshold was determined as 1.65× the mean tracer signal within the cerebellum (ce). similarly, for our simulations, we preprocess all scans by subtracting a constant background intensity (multiple of the mean tracer signal in cerebellum). as described in §2.2, pet activity in white matter or outside the brain is nonspecific and usually disregarded when interpreting tau-pet data. thus, we fit our model to match target tau-pet signal in gray matter only. while white matter regions are not penalized in the data term of (3), tau protein may still 4 http://pi.lilly.com/us/tauvid-uspi.pdf 12 klaudius scheufele et al. table 2: model calibration for adni clinical data. we report the calibrated growth parameters ρ (in [×10−3/day]) and κ (in [×10−3mm2 /day]) for seven ad patients, from the adni database. we preprocess the already smoothed pet tau values by subtracting fb× the mean signal in cerebellum from the normalized taupet scan. we run the calibration for a factor of fb = 1.6 as suggested in the tauvid protocol (highlighted in red), but also examine the sensitivity of the model calibration with respect to this parameter. patient demographics as well as mini-mental state exam (mmse) scores and patient group (alzheimer’s disease (ad), early cognitive impairment (emci), late cognitive impairment (lmci), cognitively normal (cn)) are given. the time t between scans is given in days. if available, we register mris of longitudinal snapshot, and account for atrophy (via material advection) in the personalization step. entries with “–” indicate inability to run our model because one or both of d0 and d1 are zero. fb id 022 s 6013 023 s 1190 127 s 4301 127 s 2234 035 s 4114 033 s 4179 941 s 4036 sex f f m f f m m age 60.0 83.8 76.6 64.3 57.1 85.1 74.4 mmse n/a 28 30 29 27 27 28 group ad cn emci emci lmci cn emci t [d] 401 476 477 660 426 376 464 atrophy yes yes no no yes no yes 1.0x ρ 0.00 0.41 0.00 0.00 1.95 0.62 0.00 κ 3.80e−4 4.55e−3 2.77e−3 1.71e−1 4.90e−3 1.59e−3 2.07e−2 µ 4.15e−1 7.58e−1 6.63e−1 8.99e−1 6.66e−1 6.58e−1 7.76e−1 1.2x ρ 0.04 0.69 0.00 0.91 3.15 1.58 0.14 κ 3.35e−4 3.98e−3 3.25e−3 1.67e−1 8.13e−3 2.07e−3 1.59e−2 µ 4.31e−1 8.28e−1 6.78e−1 1.03e0 7.41e−1 7.20e−1 8.94e−1 1.4x ρ 0.00 1.19 0.00 2.95 4.09 2.74 1.50 κ 1.00e−4 2.41e−3 1.83e−3 1.60e−1 6.36e−3 3.77e−3 7.37e−3 µ 4.70e−1 8.02e−1 5.99e−1 1.05e0 7.88e−1 7.51e−1 8.87e−1 1.65× ρ 3.24 1.75 0.00 3.30 4.93 4.97 3.45 κ 1.94e−3 3.24e−3 7.92e−4 8.90e−1 3.76e−3 8.39e−3 3.32e−3 µ 5.98e−1 8.31e−1 5.74e−1 1.02e0 8.22e−1 7.86e−1 9.55e−1 1.8x ρ 5.24 2.07 0.00 – 5.47 6.88 4.00 κ 2.44e−3 4.88e−3 5.70e−4 – 3.18e−3 1.28e−2 1.00e−4 µ 6.80e−1 8.88e−1 5.83e−1 – 8.42e−1 8.01e−1 9.91e−1 2.0x ρ 6.87 2.81 0.00 – 6.39 7.97 – κ 1.51e−3 1.00e−2 3.38e−4 – 3.06e−3 5.85e−3 – µ 9.27e−1 9.59e−1 6.02e−1 – 8.77e−1 8.69e−1 – spread along white matter fiber tracts. we assume a 100-fold higher migration rate in white matter, compared to gray matter. since the factor of 1.65 for the background subtraction seems somewhat arbitrary, we also study the sensitivity of the calibration (and reconstruction) with respect to this parameter. calibration results. quantitative and qualitative calibration results for varying background offsets are given in tab. 2 and figs. 2–7. looking at the suggested factor of 1.65× the mean signal in cerebellum (highlighted in red in tab. 2), we observe that the estimated growth parameters show signifi- calibration of biophysical models for tau spreading in ad 13 cant variability across different subjects. for example, we obtain values for ρ between close to zero and 4.97, and κ ranges between 8.90e−1 and 7.92e−4. this demonstrates that our model is sensitive to patient-specific information and provides an indication that the methodology could be clinically useful for differential diagnosis. second, the model calibration is quite sensitive to the offset chosen for the background subtraction. we observe large variation for the estimated growth parameters ρ and κ across different offsets for the different subjects. fig. 2 shows the calibration data and reconstruction for an offset of 1.65 × ce for the background subtraction: panel a) shows axial and sagittal cuts of the raw tau-pet (for time point t0); panels b) and c) show the background corrected tau scans (for time point t0 and t1, respectively); panels d) and e) show the initial tau seeding c(0), and reconstruction ct, respectively. figures 3–7 show calibration data and reconstruction for varying offsets used for background subtraction. for subject 035 s 4114, e.g., the estimated proliferation rate ρ varies from 1.95 to 6.39 across different offsets for the background subtraction, while the estimation of the migration rate κ is less sensitive. a similar trend can be seen for the other subjects. 4 discussion the pronounced sensitivity of the calibration parameters to the offset for the background subtraction (but also to the overall imaging preprocessing, including normalization) is partially due to the simplistic nature of our model, and its strong dependency on the initial tau seeding. the latter largely influences growth pattern and shape; since it is directly taken from the t0 tau-pet scan, the offset for background subtraction enters the model in a direct and sensitive way. reviewing figs. 3–7, we see that the actual region of interest (roi) with significant change in tau signal is typically very localized. choosing the offset too small, leaves residuals of background signal distributed throughout the brain, which, ultimately prevents the model from reconstructing (small and localized) high intensity peaks (since the larger areas of low intensity signal weigh higher in the l2 misfit of the objective function). subtracting a too large ‘background intensity‘ results in loss of information. another problem arises when rois which showed medium to low signal intensity in the t0 scan, appear with high signal intensity in the t1 scan. this is a quite common scenario, yet poses challenges on our model since the initial tau seeding likely does not have support in such regions— either because they are indistinguishable from the background signal (and removed), or because indeed the roi was not invaded by tau at t0. to capture such effects and overcome the shortcomings described above, a proper noise model must be incorporated, and parameter estimation needs to be performed in a bayesian manner. we will target this approach in a follow up study. another important model improvement is to include anisotropic diffusion, especially for data with longer time horizon t. also 14 klaudius scheufele et al. in our model, the atrophy is computed from image registration and it is only one-way coupled to tau. in our future work, we will evaluate such models [22, 41]. we remark that more complex models also require more informative data; either more time snapshots of tau or larger time intervals between scans. for this first exploratory study, however, we opted for a simple model that captures the dynamics of tau propagation and allows us to establish a calibration baseline upon which more complex models can be evaluated. recall that, although the tau propagation model is a nonlinear spatiotemporal pde, it is parametrized by only two parameters—so, in some sense it is quite minimal. yet, the model can reconstruct the basic features of clinical scans. ultimately, the reconstructed parameters κ and ρ could be used as biomarkers. finally, the model can be run forward in time to predict tau propagation and estimate ad progression, which can serve as an overall validation. indeed, having more than two scans would allow us to validate our model by using the first two scans to calibrate and the third scan to test the prediction. another challenge is the normalization of longitudinal tau pet, especially for longitudinal studies where relative change is small. the choice for a good reference region is not trivial, depends on the employed pet tracer, and is sensitive to, e.g., registration errors [23]. the pet normalization is not a trivial matter, in fact it is an ongoing research topic [13, 34]. additional challenges are linked to lack of standardization in pet image acquisition protocols. inconsistent intensity change, can be caused by several bio-physical factors such as age, weight change, and blood glucose level of the patient; or by other imaging factors such as varying scanner models or image reconstruction algorithms [4]. dealing with such challenges would require cross-validation of our method with several subjects in order to select the hyper-parameters in our scheme.